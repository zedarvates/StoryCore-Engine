"""
ComfyUI Asset Retriever
Handles downloading and managing generated assets from ComfyUI.
"""

import logging
import asyncio
import aiohttp
import hashlib
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta

from .comfyui_config import ComfyUIConfig
from .comfyui_models import ExecutionResult, AssetInfo
from .performance_monitor import PerformanceMonitor


@dataclass
class RetrievalMetrics:
    """Metrics for asset retrieval operations."""
    
    total_assets: int = 0
    successful_downloads: int = 0
    failed_downloads: int = 0
    total_bytes: int = 0
    total_time_seconds: float = 0.0
    retry_count: int = 0
    
    @property
    def success_rate(self) -> float:
        """Calculate success rate percentage."""
        if self.total_assets == 0:
            return 0.0
        return (self.successful_downloads / self.total_assets) * 100.0
    
    @property
    def average_speed_mbps(self) -> float:
        """Calculate average download speed in MB/s."""
        if self.total_time_seconds == 0:
            return 0.0
        return (self.total_bytes / (1024 * 1024)) / self.total_time_seconds


@dataclass
class CleanupPolicy:
    """Policy for cleaning up temporary files."""
    
    max_age_hours: int = 24
    max_total_size_mb: int = 1000
    keep_successful_downloads: bool = True
    keep_failed_downloads: bool = False
    cleanup_on_startup: bool = False  # Changed default to False


@dataclass
class AssetDownload:
    """Information about an asset download."""
    
    filename: str
    url: str
    local_path: Path
    size_bytes: int = 0
    checksum: Optional[str] = None
    download_time: Optional[datetime] = None
    retry_count: int = 0
    is_complete: bool = False
    error_message: Optional[str] = None


class AssetRetriever:
    """
    Retrieves and manages assets generated by ComfyUI.
    
    Handles downloading images from ComfyUI's /view endpoint, verifies file
    integrity, organizes storage, and manages cleanup policies.
    """
    
    def __init__(self, config: ComfyUIConfig, cleanup_policy: Optional[CleanupPolicy] = None):
        """
        Initialize Asset Retriever.
        
        Args:
            config: ComfyUI configuration.
            cleanup_policy: Optional cleanup policy for temporary files.
        """
        self.config = config
        self.cleanup_policy = cleanup_policy or CleanupPolicy()
        self.logger = self._setup_logging()
        
        # Performance monitor for metrics collection
        self.performance_monitor = PerformanceMonitor(config)
        
        # Storage paths
        self.assets_dir = Path("assets")
        self.temp_dir = Path("temp_assets")
        self.cache_dir = Path("asset_cache")
        
        # Create directories
        self._create_directories()
        
        # Active downloads tracking
        self._active_downloads: Dict[str, AssetDownload] = {}
        self._download_lock = asyncio.Lock()
        
        # Metrics
        self.metrics = RetrievalMetrics()
        
        # Session for HTTP requests
        self._session: Optional[aiohttp.ClientSession] = None
        
        self.logger.info(f"Asset Retriever initialized with storage at {self.assets_dir}")
    
    def _setup_logging(self) -> logging.Logger:
        """Set up logging for the retriever."""
        logger = logging.getLogger("comfyui_asset_retriever")
        logger.setLevel(getattr(logging, self.config.log_level))
        
        # Create console handler if not already exists
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _create_directories(self) -> None:
        """Create necessary directories for asset storage."""
        for directory in [self.assets_dir, self.temp_dir, self.cache_dir]:
            directory.mkdir(parents=True, exist_ok=True)
            self.logger.debug(f"Created directory: {directory}")
    
    async def __aenter__(self):
        """Async context manager entry."""
        # Use a default timeout if not available in config
        timeout = getattr(self.config, 'request_timeout', 30.0)
        self._session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=timeout)
        )
        
        # Cleanup on startup if enabled
        if self.cleanup_policy.cleanup_on_startup:
            await self._cleanup_old_files()
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self._session:
            await self._session.close()
            self._session = None
    
    async def retrieve_execution_assets(
        self,
        execution_result: ExecutionResult,
        target_directory: Optional[Path] = None
    ) -> List[AssetInfo]:
        """
        Retrieve all assets from a completed execution.
        
        Args:
            execution_result: Completed execution result with asset information.
            target_directory: Optional target directory for assets.
            
        Returns:
            List of retrieved asset information.
        """
        operation_id = f"asset_retrieval_{execution_result.prompt_id}_{int(time.time())}"
        self.performance_monitor.record_operation_start("asset_retrieval", operation_id)
        
        try:
            if not execution_result.output_images:
                self.logger.info(f"No assets to retrieve for execution {execution_result.prompt_id}")
                self.performance_monitor.record_operation_end(
                    "asset_retrieval", operation_id, True,
                    {"asset_count": 0, "prompt_id": execution_result.prompt_id}
                )
                return []
            
            target_dir = target_directory or self.assets_dir / execution_result.prompt_id
            target_dir.mkdir(parents=True, exist_ok=True)
            
            self.logger.info(f"Retrieving {len(execution_result.output_images)} assets for execution {execution_result.prompt_id}")
            
            # Download all assets concurrently
            download_tasks = []
            for image_info in execution_result.output_images:
                task = self._download_asset(image_info, target_dir)
                download_tasks.append(task)
            
            # Wait for all downloads to complete
            results = await asyncio.gather(*download_tasks, return_exceptions=True)
            
            # Process results
            retrieved_assets = []
            failed_count = 0
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.error(f"Failed to download asset {execution_result.output_images[i].filename}: {result}")
                    self.metrics.failed_downloads += 1
                    failed_count += 1
                else:
                    retrieved_assets.append(result)
                    self.metrics.successful_downloads += 1
        
            self.metrics.total_assets += len(execution_result.output_images)
            
            success_rate = len(retrieved_assets) / len(execution_result.output_images) * 100
            self.logger.info(f"Retrieved {len(retrieved_assets)}/{len(execution_result.output_images)} assets successfully")
            
            self.performance_monitor.record_operation_end(
                "asset_retrieval", operation_id, failed_count == 0,
                {
                    "total_assets": len(execution_result.output_images),
                    "successful_downloads": len(retrieved_assets),
                    "failed_downloads": failed_count,
                    "success_rate": success_rate,
                    "prompt_id": execution_result.prompt_id
                }
            )
            
            return retrieved_assets
            
        except Exception as e:
            self.logger.error(f"Asset retrieval failed: {e}")
            self.performance_monitor.record_operation_end(
                "asset_retrieval", operation_id, False,
                {"error": str(e), "prompt_id": execution_result.prompt_id}
            )
            raise
    
    async def _download_asset(self, asset_info: AssetInfo, target_dir: Path) -> AssetInfo:
        """
        Download a single asset with retry logic.
        
        Args:
            asset_info: Asset information from execution result.
            target_dir: Target directory for the asset.
            
        Returns:
            Updated asset information with local path.
        """
        start_time = time.time()
        
        # Construct download URL
        download_url = f"{self.config.server_url}/view"
        params = {
            "filename": asset_info.filename,
            "type": "output",
            "subfolder": asset_info.subfolder or ""
        }
        
        # Target file path
        target_path = target_dir / asset_info.filename
        temp_path = self.temp_dir / f"{asset_info.filename}.tmp"
        
        # Track download
        download = AssetDownload(
            filename=asset_info.filename,
            url=download_url,
            local_path=target_path
        )
        
        async with self._download_lock:
            self._active_downloads[asset_info.filename] = download
        
        try:
            # Retry logic with exponential backoff
            max_retries = 3
            base_delay = 1.0
            
            for attempt in range(max_retries + 1):
                try:
                    if not self._session:
                        raise RuntimeError("HTTP session not initialized")
                    
                    self.logger.debug(f"Downloading {asset_info.filename} (attempt {attempt + 1})")
                    
                    async with self._session.get(download_url, params=params) as response:
                        if response.status == 200:
                            # Download to temporary file first
                            total_size = 0
                            checksum = hashlib.sha256()
                            
                            with open(temp_path, 'wb') as f:
                                async for chunk in response.content.iter_chunked(8192):
                                    f.write(chunk)
                                    total_size += len(chunk)
                                    checksum.update(chunk)
                            
                            # Verify download
                            if total_size > 0:
                                # Move to final location
                                temp_path.rename(target_path)
                                
                                # Update asset info
                                asset_info.local_path = target_path
                                asset_info.size_bytes = total_size
                                asset_info.checksum = checksum.hexdigest()
                                asset_info.download_time = datetime.utcnow()
                                
                                # Update download tracking
                                download.size_bytes = total_size
                                download.checksum = asset_info.checksum
                                download.download_time = asset_info.download_time
                                download.is_complete = True
                                
                                # Update metrics
                                self.metrics.total_bytes += total_size
                                self.metrics.total_time_seconds += time.time() - start_time
                                
                                self.logger.debug(f"Successfully downloaded {asset_info.filename} ({total_size} bytes)")
                                
                                return asset_info
                            else:
                                raise ValueError("Downloaded file is empty")
                        
                        elif response.status == 404:
                            raise FileNotFoundError(f"Asset not found: {asset_info.filename}")
                        else:
                            raise aiohttp.ClientResponseError(
                                request_info=response.request_info,
                                history=response.history,
                                status=response.status,
                                message=f"HTTP {response.status}"
                            )
                
                except Exception as e:
                    download.retry_count += 1
                    self.metrics.retry_count += 1
                    
                    if attempt < max_retries:
                        delay = base_delay * (2 ** attempt)
                        self.logger.warning(f"Download attempt {attempt + 1} failed for {asset_info.filename}: {e}. Retrying in {delay}s")
                        await asyncio.sleep(delay)
                    else:
                        # Final failure
                        download.error_message = str(e)
                        self.logger.error(f"Failed to download {asset_info.filename} after {max_retries + 1} attempts: {e}")
                        raise
        
        finally:
            # Clean up temporary file
            if temp_path.exists():
                temp_path.unlink()
            
            # Remove from active downloads
            async with self._download_lock:
                self._active_downloads.pop(asset_info.filename, None)
    
    async def get_download_status(self) -> Dict[str, AssetDownload]:
        """
        Get status of all active downloads.
        
        Returns:
            Dictionary mapping filenames to download status.
        """
        async with self._download_lock:
            return dict(self._active_downloads)
    
    def organize_assets(
        self,
        source_dir: Path,
        organization_scheme: str = "by_date"
    ) -> Dict[str, Path]:
        """
        Organize assets according to specified scheme.
        
        Args:
            source_dir: Source directory containing assets.
            organization_scheme: Organization scheme ("by_date", "by_type", "by_size").
            
        Returns:
            Dictionary mapping original paths to new organized paths.
        """
        if not source_dir.exists():
            self.logger.warning(f"Source directory does not exist: {source_dir}")
            return {}
        
        organized_paths = {}
        
        for asset_file in source_dir.glob("*"):
            if asset_file.is_file():
                if organization_scheme == "by_date":
                    # Organize by creation date
                    creation_time = datetime.fromtimestamp(asset_file.stat().st_ctime)
                    date_dir = self.assets_dir / creation_time.strftime("%Y-%m-%d")
                    date_dir.mkdir(parents=True, exist_ok=True)
                    new_path = date_dir / asset_file.name
                
                elif organization_scheme == "by_type":
                    # Organize by file extension
                    extension = asset_file.suffix.lower().lstrip('.')
                    type_dir = self.assets_dir / f"{extension}_files"
                    type_dir.mkdir(parents=True, exist_ok=True)
                    new_path = type_dir / asset_file.name
                
                elif organization_scheme == "by_size":
                    # Organize by file size
                    size_mb = asset_file.stat().st_size / (1024 * 1024)
                    if size_mb < 1:
                        size_dir = self.assets_dir / "small_files"
                    elif size_mb < 10:
                        size_dir = self.assets_dir / "medium_files"
                    else:
                        size_dir = self.assets_dir / "large_files"
                    
                    size_dir.mkdir(parents=True, exist_ok=True)
                    new_path = size_dir / asset_file.name
                
                else:
                    self.logger.warning(f"Unknown organization scheme: {organization_scheme}")
                    continue
                
                # Move file if not already in target location
                if asset_file != new_path:
                    # Handle existing files by adding a counter
                    counter = 1
                    original_new_path = new_path
                    while new_path.exists():
                        stem = original_new_path.stem
                        suffix = original_new_path.suffix
                        new_path = original_new_path.parent / f"{stem}_{counter}{suffix}"
                        counter += 1
                    
                    asset_file.rename(new_path)
                    organized_paths[str(asset_file)] = new_path
                    self.logger.debug(f"Moved {asset_file.name} to {new_path}")
        
        self.logger.info(f"Organized {len(organized_paths)} assets using scheme: {organization_scheme}")
        
        return organized_paths
    
    async def verify_asset_integrity(self, asset_info: AssetInfo) -> bool:
        """
        Verify the integrity of a downloaded asset.
        
        Args:
            asset_info: Asset information with local path and checksum.
            
        Returns:
            True if asset is valid, False otherwise.
        """
        if not asset_info.local_path or not asset_info.local_path.exists():
            self.logger.warning(f"Asset file does not exist: {asset_info.local_path}")
            return False
        
        if not asset_info.checksum:
            self.logger.warning(f"No checksum available for asset: {asset_info.filename}")
            return True  # Can't verify, assume valid
        
        # Calculate current checksum
        current_checksum = hashlib.sha256()
        try:
            with open(asset_info.local_path, 'rb') as f:
                for chunk in iter(lambda: f.read(8192), b""):
                    current_checksum.update(chunk)
            
            calculated_checksum = current_checksum.hexdigest()
            
            if calculated_checksum == asset_info.checksum:
                self.logger.debug(f"Asset integrity verified: {asset_info.filename}")
                return True
            else:
                self.logger.error(f"Asset integrity check failed for {asset_info.filename}: expected {asset_info.checksum}, got {calculated_checksum}")
                return False
        
        except Exception as e:
            self.logger.error(f"Error verifying asset integrity for {asset_info.filename}: {e}")
            return False
    
    async def _cleanup_old_files(self) -> None:
        """Clean up old files according to cleanup policy."""
        self.logger.info("Starting cleanup of old files")
        
        cutoff_time = datetime.now() - timedelta(hours=self.cleanup_policy.max_age_hours)
        total_size = 0
        files_to_delete = []
        
        # Scan temporary directory
        for temp_file in self.temp_dir.glob("*"):
            if temp_file.is_file():
                file_time = datetime.fromtimestamp(temp_file.stat().st_mtime)
                file_size = temp_file.stat().st_size
                total_size += file_size
                
                # Mark for deletion if too old
                if file_time < cutoff_time:
                    files_to_delete.append(temp_file)
        
        # Check total size limit
        if total_size > self.cleanup_policy.max_total_size_mb * 1024 * 1024:
            # Sort by modification time (oldest first)
            all_temp_files = list(self.temp_dir.glob("*"))
            all_temp_files.sort(key=lambda f: f.stat().st_mtime)
            
            current_size = total_size
            for temp_file in all_temp_files:
                if current_size <= self.cleanup_policy.max_total_size_mb * 1024 * 1024:
                    break
                
                if temp_file not in files_to_delete:
                    files_to_delete.append(temp_file)
                    current_size -= temp_file.stat().st_size
        
        # Delete files
        deleted_count = 0
        for file_to_delete in files_to_delete:
            try:
                file_to_delete.unlink()
                deleted_count += 1
                self.logger.debug(f"Deleted old file: {file_to_delete}")
            except Exception as e:
                self.logger.warning(f"Failed to delete file {file_to_delete}: {e}")
        
        self.logger.info(f"Cleanup completed: deleted {deleted_count} files")
    
    def get_storage_stats(self) -> Dict[str, any]:
        """
        Get storage statistics.
        
        Returns:
            Dictionary with storage information.
        """
        stats = {
            "assets_directory": str(self.assets_dir),
            "temp_directory": str(self.temp_dir),
            "cache_directory": str(self.cache_dir),
            "total_assets": 0,
            "total_size_bytes": 0,
            "directories": {}
        }
        
        # Scan each directory
        for directory_name, directory_path in [
            ("assets", self.assets_dir),
            ("temp", self.temp_dir),
            ("cache", self.cache_dir)
        ]:
            if directory_path.exists():
                file_count = 0
                total_size = 0
                
                for file_path in directory_path.rglob("*"):
                    if file_path.is_file():
                        file_count += 1
                        total_size += file_path.stat().st_size
                
                stats["directories"][directory_name] = {
                    "file_count": file_count,
                    "size_bytes": total_size,
                    "size_mb": round(total_size / (1024 * 1024), 2)
                }
                
                stats["total_assets"] += file_count
                stats["total_size_bytes"] += total_size
        
        stats["total_size_mb"] = round(stats["total_size_bytes"] / (1024 * 1024), 2)
        
        return stats
    
    def get_metrics(self) -> RetrievalMetrics:
        """
        Get retrieval metrics.
        
        Returns:
            Current retrieval metrics.
        """
        return self.metrics
    
    def reset_metrics(self) -> None:
        """Reset retrieval metrics."""
        self.metrics = RetrievalMetrics()
        self.logger.info("Retrieval metrics reset")
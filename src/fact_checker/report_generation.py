"""
Report Generation API

This module provides functionality to generate formatted reports from
verification results, including JSON, Markdown, and PDF export formats.

Requirements: 4.5, 4.6, 4.7, 6.6
"""

import json
import hashlib
from typing import List, Dict, Any, Optional
from datetime import datetime
from .models import VerificationResult, ManipulationSignal, Report


# Standard disclaimer for all reports
STANDARD_DISCLAIMER = (
    "This report was generated by an automated fact-checking system. "
    "While the system uses trusted sources and established verification methods, "
    "automated analysis has limitations. Users should exercise critical judgment "
    "and consult additional sources for important decisions. "
    "This system does not provide medical advice, legal counsel, or financial guidance."
)


def generate_report(
    verification_results: List[VerificationResult],
    input_text: str,
    format: str = "json",
    manipulation_signals: Optional[List[ManipulationSignal]] = None
) -> Report:
    """
    Generates formatted report from verification results.
    
    The report includes:
    - Metadata (timestamp, version, input hash, processing time)
    - Detailed verification results for each claim
    - Summary statistics
    - Human-readable summary
    - Actionable recommendations
    - Standard disclaimer
    
    Args:
        verification_results: List of verification results for claims
        input_text: Original input text that was analyzed
        format: Output format ("json", "markdown", "pdf")
        manipulation_signals: Optional list of manipulation signals (for video analysis)
        
    Returns:
        Report object with structured data and human summary
        
    Examples:
        >>> results = [VerificationResult(...)]
        >>> report = generate_report(results, "Original text", format="json")
        >>> report.metadata["version"]
        '1.0'
        >>> len(report.claims) == len(results)
        True
    """
    # Generate metadata
    metadata = _generate_metadata(input_text)
    
    # Generate summary statistics
    summary_stats = _generate_summary_statistics(verification_results)
    
    # Generate human summary
    human_summary = _generate_human_summary(verification_results, manipulation_signals)
    
    # Generate recommendations
    recommendations = _generate_recommendations(verification_results, manipulation_signals)
    
    # Create report object
    report = Report(
        metadata=metadata,
        claims=verification_results,
        manipulation_signals=manipulation_signals or [],
        summary_statistics=summary_stats,
        human_summary=human_summary,
        recommendations=recommendations,
        disclaimer=STANDARD_DISCLAIMER
    )
    
    return report


def export_report_json(report: Report) -> str:
    """
    Exports report as JSON string.
    
    Args:
        report: Report object to export
        
    Returns:
        JSON string representation
    """
    # Convert report to dictionary
    report_dict = _report_to_dict(report)
    
    # Serialize to JSON with pretty printing
    return json.dumps(report_dict, indent=2, default=str)


def export_report_markdown(report: Report) -> str:
    """
    Exports report as Markdown string.
    
    Args:
        report: Report object to export
        
    Returns:
        Markdown string representation
    """
    lines = []
    
    # Title
    lines.append("# Fact-Checking Report")
    lines.append("")
    
    # Metadata section
    lines.append("## Metadata")
    lines.append(f"- **Generated**: {report.metadata['timestamp']}")
    lines.append(f"- **Version**: {report.metadata['version']}")
    lines.append(f"- **Input Hash**: {report.metadata['input_hash'][:16]}...")
    lines.append(f"- **Processing Time**: {report.metadata.get('processing_time_ms', 0)}ms")
    lines.append("")
    
    # Summary section
    lines.append("## Summary")
    lines.append(report.human_summary)
    lines.append("")
    
    # Statistics section
    lines.append("## Statistics")
    stats = report.summary_statistics
    lines.append(f"- **Total Claims**: {stats['total_claims']}")
    lines.append(f"- **High Risk Claims**: {stats['high_risk_count']}")
    lines.append(f"- **Average Confidence**: {stats['average_confidence']:.1f}%")
    lines.append("")
    
    # Risk distribution
    if 'risk_distribution' in stats:
        lines.append("### Risk Distribution")
        for risk_level, count in stats['risk_distribution'].items():
            lines.append(f"- **{risk_level.upper()}**: {count}")
        lines.append("")
    
    # Claims section
    lines.append("## Detailed Results")
    lines.append("")
    
    for i, result in enumerate(report.claims, 1):
        lines.append(f"### Claim {i}")
        lines.append(f"**Text**: {result.claim.text}")
        lines.append(f"**Confidence**: {result.confidence:.1f}%")
        lines.append(f"**Risk Level**: {result.risk_level.upper()}")
        lines.append(f"**Domain**: {result.claim.domain or 'general'}")
        lines.append("")
        lines.append(f"**Reasoning**: {result.reasoning}")
        lines.append("")
        lines.append(f"**Recommendation**: {result.recommendation}")
        lines.append("")
        
        # Evidence
        if result.supporting_evidence:
            lines.append("**Supporting Evidence**:")
            for ev in result.supporting_evidence:
                lines.append(f"- {ev.source} (credibility: {ev.credibility_score:.0f}%)")
            lines.append("")
        
        if result.contradicting_evidence:
            lines.append("**Contradicting Evidence**:")
            for ev in result.contradicting_evidence:
                lines.append(f"- {ev.source} (credibility: {ev.credibility_score:.0f}%)")
            lines.append("")
        
        lines.append("---")
        lines.append("")
    
    # Manipulation signals (if any)
    if report.manipulation_signals:
        lines.append("## Manipulation Signals")
        lines.append("")
        for signal in report.manipulation_signals:
            lines.append(f"### {signal.type.replace('_', ' ').title()}")
            lines.append(f"**Severity**: {signal.severity.upper()}")
            if signal.timestamp_start:
                lines.append(f"**Timestamp**: {signal.timestamp_start} - {signal.timestamp_end}")
            lines.append(f"**Description**: {signal.description}")
            lines.append("")
    
    # Recommendations section
    lines.append("## Recommendations")
    for i, rec in enumerate(report.recommendations, 1):
        lines.append(f"{i}. {rec}")
    lines.append("")
    
    # Disclaimer
    lines.append("## Disclaimer")
    lines.append(report.disclaimer)
    lines.append("")
    
    return "\n".join(lines)


def export_report_pdf(report: Report) -> bytes:
    """
    Exports report as PDF bytes.
    
    Note: This is a placeholder implementation. In production, this would use
    a library like ReportLab or WeasyPrint to generate actual PDFs.
    
    Args:
        report: Report object to export
        
    Returns:
        PDF bytes (currently returns markdown as bytes)
    """
    # Placeholder: convert markdown to bytes
    # In production, use a PDF generation library
    markdown = export_report_markdown(report)
    return markdown.encode('utf-8')


def _generate_metadata(input_text: str) -> Dict[str, Any]:
    """
    Generates metadata for the report.
    
    Args:
        input_text: Original input text
        
    Returns:
        Dictionary with metadata fields
    """
    # Calculate input hash
    input_hash = hashlib.sha256(input_text.encode('utf-8')).hexdigest()
    
    return {
        "timestamp": datetime.now().isoformat(),
        "version": "1.0",
        "input_hash": input_hash,
        "processing_time_ms": 0  # Will be updated by caller if needed
    }


def _generate_summary_statistics(results: List[VerificationResult]) -> Dict[str, Any]:
    """
    Generates summary statistics from verification results.
    
    Args:
        results: List of verification results
        
    Returns:
        Dictionary with summary statistics
    """
    if not results:
        return {
            "total_claims": 0,
            "high_risk_count": 0,
            "average_confidence": 0.0,
            "domains_analyzed": [],
            "risk_distribution": {}
        }
    
    # Calculate statistics
    total_claims = len(results)
    high_risk_count = sum(1 for r in results if r.risk_level in ["high", "critical"])
    average_confidence = sum(r.confidence for r in results) / total_claims
    
    # Get unique domains
    domains = set()
    for r in results:
        if r.claim.domain:
            domains.add(r.claim.domain)
    
    # Risk distribution
    risk_distribution = {}
    for r in results:
        risk_distribution[r.risk_level] = risk_distribution.get(r.risk_level, 0) + 1
    
    return {
        "total_claims": total_claims,
        "high_risk_count": high_risk_count,
        "average_confidence": round(average_confidence, 2),
        "domains_analyzed": sorted(list(domains)),
        "risk_distribution": risk_distribution
    }


def _generate_human_summary(
    results: List[VerificationResult],
    manipulation_signals: Optional[List[ManipulationSignal]] = None
) -> str:
    """
    Generates human-readable summary of verification results.
    
    Args:
        results: List of verification results
        manipulation_signals: Optional manipulation signals
        
    Returns:
        Human-readable summary string
    """
    if not results:
        return "No claims were analyzed."
    
    parts = []
    
    # Overall assessment
    total = len(results)
    high_risk = sum(1 for r in results if r.risk_level in ["high", "critical"])
    avg_confidence = sum(r.confidence for r in results) / total
    
    parts.append(f"Analyzed {total} factual claim(s) from the provided content.")
    
    # Confidence assessment
    if avg_confidence >= 80:
        parts.append(f"Overall confidence is high ({avg_confidence:.1f}%), indicating well-supported claims.")
    elif avg_confidence >= 60:
        parts.append(f"Overall confidence is moderate ({avg_confidence:.1f}%), suggesting mixed evidence quality.")
    else:
        parts.append(f"Overall confidence is low ({avg_confidence:.1f}%), indicating insufficient supporting evidence.")
    
    # Risk assessment
    if high_risk == 0:
        parts.append("No high-risk claims were identified.")
    elif high_risk == 1:
        parts.append("⚠️ 1 high-risk claim requires attention.")
    else:
        parts.append(f"⚠️ {high_risk} high-risk claims require attention.")
    
    # Domain breakdown
    domains = {}
    for r in results:
        domain = r.claim.domain or "general"
        domains[domain] = domains.get(domain, 0) + 1
    
    if len(domains) > 1:
        domain_list = ", ".join(f"{d} ({c})" for d, c in sorted(domains.items()))
        parts.append(f"Claims span multiple domains: {domain_list}.")
    
    # Manipulation signals
    if manipulation_signals:
        high_severity = sum(1 for s in manipulation_signals if s.severity == "high")
        if high_severity > 0:
            parts.append(f"⚠️ Detected {high_severity} high-severity manipulation signal(s).")
    
    return " ".join(parts)


def _generate_recommendations(
    results: List[VerificationResult],
    manipulation_signals: Optional[List[ManipulationSignal]] = None
) -> List[str]:
    """
    Generates actionable recommendations based on verification results.
    
    Args:
        results: List of verification results
        manipulation_signals: Optional manipulation signals
        
    Returns:
        List of recommendation strings
    """
    recommendations = []
    
    if not results:
        return ["No claims to analyze. Provide content with factual assertions."]
    
    # Check for critical claims
    critical = [r for r in results if r.risk_level == "critical"]
    if critical:
        recommendations.append(
            f"URGENT: Remove or revise {len(critical)} critical claim(s) with very low confidence."
        )
    
    # Check for high-risk claims
    high_risk = [r for r in results if r.risk_level == "high"]
    if high_risk:
        recommendations.append(
            f"Verify {len(high_risk)} high-risk claim(s) with additional authoritative sources."
        )
    
    # Check for medium-risk claims
    medium_risk = [r for r in results if r.risk_level == "medium"]
    if medium_risk:
        recommendations.append(
            f"Consider adding disclaimers or qualifying language for {len(medium_risk)} medium-risk claim(s)."
        )
    
    # Check for claims lacking evidence
    no_evidence = [r for r in results if not r.supporting_evidence]
    if no_evidence:
        recommendations.append(
            f"Find supporting sources for {len(no_evidence)} claim(s) with no evidence."
        )
    
    # Manipulation signal recommendations
    if manipulation_signals:
        high_severity = [s for s in manipulation_signals if s.severity == "high"]
        if high_severity:
            recommendations.append(
                f"Review {len(high_severity)} segment(s) flagged for high-severity manipulation."
            )
    
    # General recommendations
    avg_confidence = sum(r.confidence for r in results) / len(results)
    if avg_confidence < 70:
        recommendations.append(
            "Overall confidence is below threshold. Consider comprehensive fact-checking review."
        )
    
    # If everything looks good
    if not recommendations:
        recommendations.append(
            "All claims have acceptable confidence levels. Proceed with proper source attribution."
        )
    
    return recommendations


def _report_to_dict(report: Report) -> Dict[str, Any]:
    """
    Converts Report object to dictionary for JSON serialization.
    
    Args:
        report: Report object
        
    Returns:
        Dictionary representation
    """
    return {
        "metadata": report.metadata,
        "claims": [
            {
                "claim": {
                    "id": r.claim.id,
                    "text": r.claim.text,
                    "position": r.claim.position,
                    "domain": r.claim.domain
                },
                "confidence": r.confidence,
                "risk_level": r.risk_level,
                "supporting_evidence": [
                    {
                        "source": e.source,
                        "source_type": e.source_type,
                        "credibility_score": e.credibility_score,
                        "relevance": e.relevance,
                        "excerpt": e.excerpt,
                        "url": e.url
                    }
                    for e in r.supporting_evidence
                ],
                "contradicting_evidence": [
                    {
                        "source": e.source,
                        "source_type": e.source_type,
                        "credibility_score": e.credibility_score,
                        "relevance": e.relevance,
                        "excerpt": e.excerpt,
                        "url": e.url
                    }
                    for e in r.contradicting_evidence
                ],
                "reasoning": r.reasoning,
                "recommendation": r.recommendation
            }
            for r in report.claims
        ],
        "manipulation_signals": [
            {
                "type": s.type,
                "severity": s.severity,
                "timestamp_start": s.timestamp_start,
                "timestamp_end": s.timestamp_end,
                "description": s.description,
                "evidence": s.evidence,
                "confidence": s.confidence
            }
            for s in report.manipulation_signals
        ],
        "summary_statistics": report.summary_statistics,
        "human_summary": report.human_summary,
        "recommendations": report.recommendations,
        "disclaimer": report.disclaimer
    }


def save_report_to_file(report: Report, filepath: str, format: str = "json") -> None:
    """
    Saves report to file in specified format.
    
    Args:
        report: Report object to save
        filepath: Path to save file to
        format: Format to save in ("json", "markdown", "pdf")
        
    Raises:
        ValueError: If format is not supported
    """
    if format == "json":
        content = export_report_json(report)
        mode = 'w'
    elif format == "markdown":
        content = export_report_markdown(report)
        mode = 'w'
    elif format == "pdf":
        content = export_report_pdf(report)
        mode = 'wb'
    else:
        raise ValueError(f"Unsupported format: {format}")
    
    with open(filepath, mode) as f:
        if isinstance(content, bytes):
            f.write(content)
        else:
            f.write(content)

# ‚úÖ Correction Endpoint Ollama - Guide Complet

## üéØ Probl√®me R√©solu

Le probl√®me principal √©tait que `llmService.ts` utilisait le mauvais endpoint Ollama:
- ‚ùå **Ancien**: `/api/chat` (n'existe pas dans Ollama)
- ‚úÖ **Nouveau**: `/api/generate` (endpoint correct)

## üìù Changements Effectu√©s

### 1. Correction de l'Endpoint dans `llmService.ts`

**Fichier**: `creative-studio-ui/src/services/llmService.ts`

#### M√©thode `generateCompletion()` (ligne ~650)
```typescript
// AVANT (INCORRECT)
const response = await fetch(`${endpoint}/api/chat`, {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: this.config.model,
    messages: [
      ...(request.systemPrompt ? [{ role: 'system', content: request.systemPrompt }] : []),
      { role: 'user', content: request.prompt },
    ],
    stream: false,
  }),
});

// APR√àS (CORRECT)
const response = await fetch(`${endpoint}/api/generate`, {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: this.config.model,
    prompt: request.systemPrompt 
      ? `${request.systemPrompt}\n\n${request.prompt}`
      : request.prompt,
    stream: false,
    options: {
      temperature: request.temperature ?? this.config.parameters.temperature,
      num_predict: request.maxTokens ?? this.config.parameters.maxTokens,
    },
  }),
});
```

#### M√©thode `generateStreamingCompletion()` (ligne ~700)
```typescript
// AVANT (INCORRECT)
const response = await fetch(`${endpoint}/api/chat`, {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: this.config.model,
    messages: [...],
    stream: true,
  }),
});

// APR√àS (CORRECT)
const response = await fetch(`${endpoint}/api/generate`, {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: this.config.model,
    prompt: request.systemPrompt 
      ? `${request.systemPrompt}\n\n${request.prompt}`
      : request.prompt,
    stream: true,
    options: {
      temperature: request.temperature ?? this.config.parameters.temperature,
      num_predict: request.maxTokens ?? this.config.parameters.maxTokens,
    },
  }),
});
```

#### Parsing de la R√©ponse
```typescript
// AVANT (INCORRECT)
const data = await response.json();
return {
  content: data.message?.content || '',
  finishReason: data.done ? 'stop' : 'length',
};

// APR√àS (CORRECT)
const data = await response.json();
return {
  content: data.response || '', // Ollama utilise 'response', pas 'message.content'
  finishReason: data.done ? 'stop' : 'length',
};
```

## üîß √âtapes pour Appliquer la Correction

### √âtape 1: Nettoyer le Cache du Navigateur

**Option A: Hard Refresh (Recommand√©)**
```
Windows/Linux: Ctrl + Shift + R
Mac: Cmd + Shift + R
```

**Option B: Vider le Cache Manuellement**
1. Ouvrir DevTools (F12)
2. Aller dans l'onglet "Application" ou "Storage"
3. Cliquer sur "Clear storage" ou "Vider le stockage"
4. Cocher toutes les cases
5. Cliquer sur "Clear site data"

### √âtape 2: Nettoyer localStorage

**Ouvrir la Console du Navigateur (F12) et ex√©cuter:**
```javascript
// Supprimer toutes les anciennes configurations
localStorage.removeItem('storycore_llm_config');
localStorage.removeItem('storycore_api_key_enc');
localStorage.removeItem('storycore-settings');
localStorage.removeItem('llm-config');

// V√©rifier que c'est bien supprim√©
console.log('Nettoyage termin√©!');
console.log('llm_config:', localStorage.getItem('storycore_llm_config'));
console.log('settings:', localStorage.getItem('storycore-settings'));
```

### √âtape 3: Red√©marrer le Serveur de D√©veloppement

**Dans le terminal:**
```bash
# Arr√™ter le serveur (Ctrl+C)
# Puis red√©marrer
cd creative-studio-ui
npm run dev
```

### √âtape 4: Recharger la Page

1. Fermer tous les onglets de StoryCore
2. Ouvrir un nouvel onglet
3. Aller sur `http://localhost:5173`
4. Faire un Hard Refresh (Ctrl+Shift+R)

### √âtape 5: Reconfigurer le LLM

1. Cliquer sur l'ic√¥ne ‚öôÔ∏è Settings dans le chatbox
2. S√©lectionner "Local LLM" comme provider
3. V√©rifier que l'endpoint est: `http://localhost:11434`
4. S√©lectionner un mod√®le disponible (ex: `llama3.1:8b`)
5. Cliquer sur "Test Connection"
6. Si √ßa fonctionne ‚úÖ, cliquer sur "Save Settings"

## üß™ V√©rification que √ßa Fonctionne

### Test 1: V√©rifier l'Endpoint dans DevTools

1. Ouvrir DevTools (F12)
2. Aller dans l'onglet "Network"
3. Envoyer un message dans le chatbox
4. Chercher la requ√™te vers Ollama
5. V√©rifier que l'URL est: `http://localhost:11434/api/generate` ‚úÖ
6. Si c'est encore `/api/chat` ‚ùå, refaire les √©tapes 1-4

### Test 2: V√©rifier la R√©ponse

Dans la console du navigateur, vous devriez voir:
```
[LLMConfigService] Initialized successfully
[LLMConfigService] Auto-detected model: llama3.1:8b
```

Et PAS:
```
POST http://localhost:11434/api/chat 404 (Not Found)
model 'local-model' not found
```

### Test 3: Envoyer un Message

1. Taper "Bonjour" dans le chatbox
2. Appuyer sur Entr√©e
3. Vous devriez voir:
   - ‚úÖ Le message s'affiche
   - ‚úÖ L'assistant r√©pond en streaming
   - ‚úÖ Pas d'erreur dans la console

## üêõ D√©pannage

### Probl√®me: Toujours l'erreur 404 sur /api/chat

**Solution:**
```bash
# 1. V√©rifier que les changements sont bien dans le fichier
cat creative-studio-ui/src/services/llmService.ts | grep "/api/generate"

# 2. Si rien ne s'affiche, le fichier n'a pas √©t√© modifi√©
# Refaire les modifications manuellement

# 3. Red√©marrer le serveur
cd creative-studio-ui
npm run dev
```

### Probl√®me: "model 'local-model' not found"

**Solution:**
```javascript
// Dans la console du navigateur
localStorage.clear();
location.reload();
```

### Probl√®me: "Cannot read properties of null (reading 'provider')"

**Solution:**
1. Ouvrir Settings ‚Üí LLM Configuration
2. Configurer le provider
3. Tester la connexion
4. Sauvegarder

### Probl√®me: Le code ne se recharge pas

**Solution:**
```bash
# 1. Arr√™ter le serveur (Ctrl+C)
# 2. Supprimer le cache de build
rm -rf creative-studio-ui/node_modules/.vite
rm -rf creative-studio-ui/dist

# 3. Red√©marrer
cd creative-studio-ui
npm run dev
```

## üìä Diff√©rences API Ollama

### Format `/api/chat` (N'EXISTE PAS)
```json
{
  "model": "llama3.1:8b",
  "messages": [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "..."}
  ]
}
```

### Format `/api/generate` (CORRECT) ‚úÖ
```json
{
  "model": "llama3.1:8b",
  "prompt": "System: ...\n\nUser: ...",
  "stream": true,
  "options": {
    "temperature": 0.7,
    "num_predict": 2000
  }
}
```

### R√©ponse `/api/generate`
```json
{
  "model": "llama3.1:8b",
  "created_at": "2024-01-20T...",
  "response": "Bonjour! Comment puis-je vous aider?",
  "done": false
}
```

## ‚úÖ Checklist de V√©rification

- [ ] Le fichier `llmService.ts` contient `/api/generate` (pas `/api/chat`)
- [ ] Le serveur de dev a √©t√© red√©marr√©
- [ ] Le cache du navigateur a √©t√© vid√© (Ctrl+Shift+R)
- [ ] localStorage a √©t√© nettoy√©
- [ ] La page a √©t√© recharg√©e
- [ ] Le LLM a √©t√© reconfigur√© dans Settings
- [ ] La connexion a √©t√© test√©e avec succ√®s
- [ ] Un message de test a √©t√© envoy√© et a re√ßu une r√©ponse

## üéâ R√©sultat Attendu

Apr√®s avoir suivi toutes les √©tapes:

1. ‚úÖ Le chatbox affiche "Assistant StoryCore" avec statut "Online"
2. ‚úÖ Le mod√®le s√©lectionn√© s'affiche (ex: "llama3.1:8b")
3. ‚úÖ Les messages envoy√©s re√ßoivent des r√©ponses en streaming
4. ‚úÖ Aucune erreur 404 dans la console
5. ‚úÖ Aucune erreur "model not found"

## üìû Si √áa Ne Fonctionne Toujours Pas

1. V√©rifier qu'Ollama est bien lanc√©:
   ```bash
   curl http://localhost:11434/api/tags
   ```

2. V√©rifier que le mod√®le existe:
   ```bash
   ollama list
   ```

3. Tester manuellement l'API:
   ```bash
   curl http://localhost:11434/api/generate -d '{
     "model": "llama3.1:8b",
     "prompt": "Hello",
     "stream": false
   }'
   ```

4. Si tout fonctionne en ligne de commande mais pas dans l'app:
   - V√©rifier les CORS
   - V√©rifier le firewall
   - Red√©marrer Ollama

---

**Date**: 2026-01-20
**Statut**: ‚úÖ Correction Appliqu√©e
**Fichiers Modifi√©s**: `creative-studio-ui/src/services/llmService.ts`

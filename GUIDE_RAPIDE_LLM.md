# Guide Rapide - Activer l'Aide LLM dans Project Setup

## Probl√®me
Le bouton "Suggest Name" ‚ú® dans le wizard Project Setup ne fonctionne pas.

## Solution Rapide (5 minutes)

### √âtape 1: Installer Ollama (si pas d√©j√† fait)

**Windows**:
1. T√©l√©charger: https://ollama.ai/download
2. Installer l'application
3. Lancer Ollama Desktop

**Ou via ligne de commande**:
```bash
winget install Ollama.Ollama
```

### √âtape 2: T√©l√©charger un Mod√®le

Ouvrir un terminal et ex√©cuter:
```bash
ollama pull llama3.2
```

Attendre le t√©l√©chargement (quelques minutes selon votre connexion).

### √âtape 3: D√©marrer Ollama

```bash
ollama serve
```

Ou simplement lancer l'application Ollama Desktop.

**V√©rifier que √ßa fonctionne**:
- Ouvrir un navigateur: http://localhost:11434
- Devrait afficher: "Ollama is running"

### √âtape 4: Configurer dans StoryCore

1. Dans StoryCore, ouvrir **Menu > Settings > LLM Settings**
2. Remplir:
   - **Provider**: Ollama
   - **Endpoint**: `http://localhost:11434`
   - **Model**: `llama3.2`
3. Cliquer **"Test Connection"**
   - Devrait afficher "Connected" en vert ‚úÖ
4. Cliquer **"Save"**

### √âtape 5: Utiliser dans le Wizard

1. Retourner au **wizard Project Setup**
2. **S√©lectionner au moins**:
   - Un **Genre** (ex: Fantasy)
   - Un **Tone** (ex: Epic)
3. Cliquer **"Suggest Name" ‚ú®**
4. Attendre quelques secondes
5. Le champ "Project Name" se remplit automatiquement! üéâ

## V√©rification Rapide

Si le bouton est toujours gris√©, v√©rifiez:

‚úÖ **Ollama est lanc√©**
```bash
# Tester dans un terminal:
curl http://localhost:11434
# Devrait r√©pondre: "Ollama is running"
```

‚úÖ **Au moins un Genre s√©lectionn√©** (cochez une case dans Genre)

‚úÖ **Au moins un Tone s√©lectionn√©** (cochez une case dans Tone)

‚úÖ **LLM configur√© dans Settings**
- Menu > Settings > LLM Settings
- Test Connection = vert ‚úÖ

## D√©pannage

### Le bouton reste gris√©
‚Üí V√©rifiez que Genre ET Tone sont s√©lectionn√©s

### Message "LLM service not configured"
‚Üí Allez dans Settings > LLM et configurez Ollama

### Erreur "Failed to connect"
‚Üí V√©rifiez qu'Ollama est lanc√©: `ollama serve`

### Erreur "Model not found"
‚Üí T√©l√©chargez le mod√®le: `ollama pull llama3.2`

## Alternative: Utiliser OpenAI

Si vous pr√©f√©rez utiliser OpenAI:

1. Obtenir une cl√© API: https://platform.openai.com/api-keys
2. Dans StoryCore > Settings > LLM:
   - Provider: **OpenAI**
   - API Key: **votre-cl√©-api**
   - Model: **gpt-4** ou **gpt-3.5-turbo**
3. Test Connection ‚Üí Save
4. Retourner au wizard et r√©essayer

---

**Temps estim√©**: 5-10 minutes
**Difficult√©**: Facile
**Co√ªt**: Gratuit (Ollama) ou payant (OpenAI)

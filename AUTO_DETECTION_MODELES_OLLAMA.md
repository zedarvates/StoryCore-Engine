# Auto-D√©tection des Mod√®les Ollama

## üéØ Am√©lioration Majeure

StoryCore d√©tecte maintenant **automatiquement** les mod√®les Ollama install√©s sur votre machine au lieu d'utiliser des noms hardcod√©s!

## ‚úÖ Comment √áa Fonctionne

### Au Premier Lancement

1. **Interrogation d'Ollama**: StoryCore appelle `http://localhost:11434/api/tags`
2. **R√©cup√©ration de la liste**: Obtient tous les mod√®les install√©s
3. **S√©lection intelligente**: Choisit le meilleur mod√®le selon un ordre de priorit√©
4. **Configuration automatique**: Sauvegarde le mod√®le d√©tect√©

### Ordre de Priorit√©

StoryCore cherche les mod√®les dans cet ordre:

1. `llama3.2:1b` - Ultra l√©ger et rapide
2. `llama3.2:3b` - Bon √©quilibre
3. `phi3:mini` - Performant
4. `llama3.1:8b` - Haute qualit√©
5. `mistral:7b` - Tr√®s bon
6. `gemma2:9b` - Gemma 2
7. `qwen2.5:7b` - Qwen
8. **Premier mod√®le disponible** - Si aucun des pr√©f√©r√©s n'est trouv√©

### Si Aucun Mod√®le Trouv√©

- Utilise `llama3.2:1b` comme fallback
- Affiche un message clair pour installer un mod√®le
- L'utilisateur peut changer dans Settings

## üìã Exemple de Logs

```
[LLMConfigService] Initializing...
[LLMConfigService] No configuration found, detecting available models...
[OllamaDetection] Fetching models from http://localhost:11434/api/tags
[LLMConfigService] Auto-detected model: gemma3:1b
[LLMConfigService] Reason: Found gemma3:1b installed - good balance of speed and quality
[LLMConfigService] Alternatives: llama3.2:3b, mistral:7b, phi3:mini
[LLMConfigService] LLM service created
[LLMConfigService] Configuration saved to storage
[LLMConfigService] Initialized successfully
```

## üîß Fonctions Disponibles

### `getInstalledOllamaModels()`
R√©cup√®re la liste compl√®te des mod√®les install√©s.

```typescript
const models = await getInstalledOllamaModels();
// Returns: [{ name: 'gemma3:1b', size: '1.6GB', modified: '2024-01-20' }, ...]
```

### `suggestBestModel()`
Sugg√®re le meilleur mod√®le √† utiliser.

```typescript
const suggestion = await suggestBestModel();
// Returns: { 
//   model: 'gemma3:1b', 
//   reason: 'Found gemma3:1b installed...', 
//   alternatives: ['llama3.2:3b', 'mistral:7b'] 
// }
```

### `isModelInstalled()`
V√©rifie si un mod√®le sp√©cifique est install√©.

```typescript
const installed = await isModelInstalled('gemma3:1b');
// Returns: true or false
```

### `validateModelName()`
Valide un nom de mod√®le et donne des suggestions.

```typescript
const result = await validateModelName('gemma2:2b');
// Returns: { 
//   valid: false, 
//   message: 'Model gemma2:2b not found. Available: gemma3:1b, llama3.2:3b' 
// }
```

## üéØ Avantages

### 1. Plus de Noms Hardcod√©s
- ‚úÖ Fonctionne avec **n'importe quel mod√®le** Ollama
- ‚úÖ Supporte les **nouveaux mod√®les** automatiquement
- ‚úÖ Pas besoin de mettre √† jour le code

### 2. D√©tection Intelligente
- ‚úÖ Choisit le meilleur mod√®le disponible
- ‚úÖ Ordre de priorit√© optimis√© (vitesse/qualit√©)
- ‚úÖ Fallback gracieux si aucun mod√®le pr√©f√©r√©

### 3. Exp√©rience Utilisateur
- ‚úÖ Configuration automatique au premier lancement
- ‚úÖ Messages clairs dans les logs
- ‚úÖ Suggestions d'alternatives

### 4. Flexibilit√©
- ‚úÖ Fonctionne avec Gemma 3 si vous l'avez
- ‚úÖ Fonctionne avec Llama 3.2, 3.1
- ‚úÖ Fonctionne avec n'importe quel mod√®le Ollama

## üìä Cas d'Usage

### Cas 1: Utilisateur avec Gemma 3
```
Mod√®les install√©s: gemma3:1b, gemma3:4b
R√©sultat: Utilise gemma3:1b (d√©tect√© automatiquement)
```

### Cas 2: Utilisateur avec Llama 3.2
```
Mod√®les install√©s: llama3.2:1b, llama3.2:3b
R√©sultat: Utilise llama3.2:1b (priorit√© 1)
```

### Cas 3: Utilisateur avec Mod√®les Vari√©s
```
Mod√®les install√©s: mistral:7b, qwen2.5:7b, phi3:mini
R√©sultat: Utilise phi3:mini (priorit√© 3)
```

### Cas 4: Aucun Mod√®le Install√©
```
Mod√®les install√©s: (aucun)
R√©sultat: Utilise llama3.2:1b comme fallback
Message: "No models detected, using fallback"
```

## üîÑ Int√©gration dans l'Interface

### Settings ‚Üí LLM Configuration

Le dropdown des mod√®les peut maintenant √™tre peupl√© dynamiquement:

```typescript
import { getModelNames } from '@/utils/ollamaModelDetection';

// Dans le composant
const [availableModels, setAvailableModels] = useState<string[]>([]);

useEffect(() => {
  async function loadModels() {
    const models = await getModelNames();
    setAvailableModels(models);
  }
  loadModels();
}, []);

// Dans le JSX
<select>
  {availableModels.map(model => (
    <option key={model} value={model}>{model}</option>
  ))}
</select>
```

## ‚ö†Ô∏è Notes Importantes

### Timeout
- La d√©tection a un timeout de 5 secondes
- Si Ollama ne r√©pond pas, utilise le fallback

### Endpoint
- Par d√©faut: `http://localhost:11434`
- Peut √™tre configur√© dans Settings

### Ordre de Priorit√©
- Bas√© sur un √©quilibre vitesse/qualit√©
- Peut √™tre modifi√© dans `ollamaModelDetection.ts`

## üöÄ Prochaines √âtapes

### Am√©lioration 1: Rafra√Æchir la Liste
Ajouter un bouton "Refresh Models" dans Settings pour recharger la liste.

### Am√©lioration 2: Validation en Temps R√©el
Valider le mod√®le s√©lectionn√© avant de sauvegarder.

### Am√©lioration 3: T√©l√©chargement Int√©gr√©
Ajouter un bouton "Download Model" directement dans l'interface.

### Am√©lioration 4: Informations sur les Mod√®les
Afficher la taille, la date de modification, etc.

## üìö Fichiers Modifi√©s

1. **`creative-studio-ui/src/utils/ollamaModelDetection.ts`**
   - Utilitaire de d√©tection (d√©j√† cr√©√©)

2. **`creative-studio-ui/src/services/llmConfigService.ts`**
   - Int√©gration de la d√©tection automatique
   - Logs d√©taill√©s

## üí° R√©sum√©

**Avant**:
```typescript
model: 'gemma2:2b' // Hardcod√©, ne fonctionne pas si pas install√©
```

**Apr√®s**:
```typescript
// D√©tection automatique
const suggestion = await suggestBestModel();
model: suggestion.model // Utilise ce qui est r√©ellement install√©
```

---

**Date**: 2026-01-20  
**Statut**: ‚úÖ Impl√©ment√©  
**Impact**: Majeur - R√©sout tous les probl√®mes de mod√®les inexistants

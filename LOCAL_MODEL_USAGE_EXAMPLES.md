# Exemples d'Utilisation - Gestion des Mod√®les Locaux

## üìö Table des mati√®res

1. [Utilisation de base](#utilisation-de-base)
2. [Utilisation avanc√©e](#utilisation-avanc√©e)
3. [Int√©gration dans l'application](#int√©gration-dans-lapplication)
4. [Cas d'usage r√©els](#cas-dusage-r√©els)
5. [D√©pannage](#d√©pannage)

## Utilisation de base

### Exemple 1: Premier t√©l√©chargement

```typescript
// L'utilisateur ouvre les param√®tres LLM
// S√©lectionne "Local" comme provider
// Le composant LocalModelSelector s'affiche automatiquement

// Interaction utilisateur:
// 1. Clic sur "Download" pour Gemma 3 1B
// 2. Attente du t√©l√©chargement (barre de progression)
// 3. Mod√®le automatiquement s√©lectionn√©
// 4. Clic sur "Save Settings"

// R√©sultat:
// - Mod√®le t√©l√©charg√© dans ~/.ollama/models/
// - Configuration LLM mise √† jour avec model: "gemma3:1b"
// - Pr√™t √† utiliser dans l'application
```

### Exemple 2: S√©lection d'un mod√®le existant

```typescript
// L'utilisateur a d√©j√† plusieurs mod√®les install√©s
// Ouvre les param√®tres LLM

// Interaction utilisateur:
// 1. Voir les mod√®les avec badge "‚úì Installed"
// 2. Clic sur "Select" pour Llama 3 8B
// 3. Clic sur "Save Settings"

// R√©sultat:
// - Configuration mise √† jour avec model: "llama3:8b"
// - Changement imm√©diat pour les prochaines g√©n√©rations
```

### Exemple 3: Suppression d'un mod√®le

```typescript
// L'utilisateur veut lib√©rer de l'espace disque

// Interaction utilisateur:
// 1. Clic sur l'ic√¥ne üóëÔ∏è pour Gemma 3 7B
// 2. Confirmation: "Are you sure...?"
// 3. Mod√®le supprim√©

// R√©sultat:
// - Mod√®le supprim√© de ~/.ollama/models/
// - Espace disque lib√©r√© (7GB)
// - Badge "Installed" dispara√Æt
// - Si c'√©tait le mod√®le s√©lectionn√©, s√©lection effac√©e
```

## Utilisation avanc√©e

### Exemple 4: Utilisation programmatique du service

```typescript
import { getLocalModelService } from '@/services/localModelService';

async function setupLocalModel() {
  const modelService = getLocalModelService('http://localhost:11434');
  
  // V√©rifier si Ollama est en cours d'ex√©cution
  const isRunning = await modelService.isOllamaRunning();
  if (!isRunning) {
    console.error('Ollama is not running');
    return;
  }
  
  // Obtenir les mod√®les install√©s
  const installed = await modelService.getInstalledModels();
  console.log('Installed models:', installed);
  
  // Obtenir les recommandations
  const recommended = await modelService.getRecommendedModels();
  console.log('Recommended models:', recommended);
  
  // T√©l√©charger un mod√®le avec suivi de progression
  const success = await modelService.downloadModel(
    'gemma3:3b',
    (progress) => {
      console.log(`Download progress: ${progress.progress.toFixed(2)}%`);
      console.log(`Downloaded: ${modelService.formatBytes(progress.downloadedBytes)}`);
      console.log(`Total: ${modelService.formatBytes(progress.totalBytes)}`);
    }
  );
  
  if (success) {
    console.log('Model downloaded successfully!');
  }
}
```

### Exemple 5: Int√©gration dans un composant React

```typescript
import { useState, useEffect } from 'react';
import { getLocalModelService, type LocalModel } from '@/services/localModelService';

function MyModelManager() {
  const [models, setModels] = useState<LocalModel[]>([]);
  const [selectedModel, setSelectedModel] = useState<string>('');
  const [isLoading, setIsLoading] = useState(true);
  
  const modelService = getLocalModelService();
  
  useEffect(() => {
    loadModels();
  }, []);
  
  const loadModels = async () => {
    setIsLoading(true);
    
    // Obtenir les mod√®les recommand√©s
    const recommended = await modelService.getRecommendedModels();
    setModels(recommended);
    
    // Obtenir les mod√®les install√©s
    const installed = await modelService.getInstalledModels();
    
    // S√©lectionner le premier mod√®le install√©
    if (installed.length > 0) {
      setSelectedModel(installed[0]);
    }
    
    setIsLoading(false);
  };
  
  const handleDownload = async (modelId: string) => {
    await modelService.downloadModel(modelId, (progress) => {
      // Mettre √† jour l'UI avec la progression
      console.log(`Downloading ${modelId}: ${progress.progress}%`);
    });
    
    // Recharger les mod√®les
    await loadModels();
  };
  
  return (
    <div>
      {isLoading ? (
        <p>Loading models...</p>
      ) : (
        <div>
          {models.map(model => (
            <div key={model.id}>
              <h3>{model.displayName}</h3>
              <p>{model.description}</p>
              <button onClick={() => handleDownload(model.id)}>
                Download
              </button>
            </div>
          ))}
        </div>
      )}
    </div>
  );
}
```

### Exemple 6: D√©tection des capacit√©s syst√®me

```typescript
import { getLocalModelService } from '@/services/localModelService';

async function analyzeSystemCapabilities() {
  const modelService = getLocalModelService();
  
  // Obtenir les capacit√©s syst√®me
  const capabilities = await modelService.getSystemCapabilities();
  
  console.log('System Analysis:');
  console.log(`Total RAM: ${capabilities.totalRAM}GB`);
  console.log(`Available RAM: ${capabilities.availableRAM}GB`);
  console.log(`Has GPU: ${capabilities.hasGPU}`);
  
  // Obtenir le meilleur mod√®le pour ce syst√®me
  const bestModel = await modelService.getBestModel();
  
  if (bestModel) {
    console.log('\nRecommended Model:');
    console.log(`Name: ${bestModel.displayName}`);
    console.log(`Size: ${bestModel.size}`);
    console.log(`Min RAM: ${bestModel.minRAM}GB`);
    console.log(`Recommended RAM: ${bestModel.recommendedRAM}GB`);
    console.log(`Requires GPU: ${bestModel.requiresGPU}`);
  }
}
```

## Int√©gration dans l'application

### Exemple 7: Utilisation dans un wizard

```typescript
import { getLocalModelService } from '@/services/localModelService';
import { getLLMService } from '@/services/llmService';

async function generateWithLocalModel(prompt: string) {
  const modelService = getLocalModelService();
  
  // V√©rifier qu'un mod√®le est install√©
  const installed = await modelService.getInstalledModels();
  if (installed.length === 0) {
    throw new Error('No local models installed');
  }
  
  // Utiliser le premier mod√®le install√©
  const modelId = installed[0];
  
  // Cr√©er le service LLM avec le mod√®le local
  const llmService = getLLMService({
    provider: 'local',
    apiEndpoint: 'http://localhost:11434',
    model: modelId,
    apiKey: '', // Pas n√©cessaire pour local
    parameters: {
      temperature: 0.7,
      maxTokens: 2000,
      topP: 1.0,
      frequencyPenalty: 0,
      presencePenalty: 0,
    },
    systemPrompts: {
      worldGeneration: 'You are a creative world-building assistant...',
      characterGeneration: 'You are a character development expert...',
      dialogueGeneration: 'You are a dialogue writing specialist...',
    },
    timeout: 30000,
    retryAttempts: 3,
    streamingEnabled: true,
  });
  
  // G√©n√©rer du contenu
  const response = await llmService.generateCompletion({
    prompt,
    systemPrompt: 'You are a helpful assistant.',
  });
  
  if (response.success && response.data) {
    return response.data.content;
  }
  
  throw new Error('Generation failed');
}

// Utilisation
const worldDescription = await generateWithLocalModel(
  'Create a fantasy world with magic and dragons'
);
console.log(worldDescription);
```

### Exemple 8: Chatbox avec mod√®le local

```typescript
import { useState } from 'react';
import { getLocalModelService } from '@/services/localModelService';
import { getLLMService } from '@/services/llmService';

function LocalChatbox() {
  const [messages, setMessages] = useState<Array<{role: string, content: string}>>([]);
  const [input, setInput] = useState('');
  const [selectedModel, setSelectedModel] = useState('gemma3:3b');
  const [isGenerating, setIsGenerating] = useState(false);
  
  const handleSend = async () => {
    if (!input.trim()) return;
    
    // Ajouter le message utilisateur
    const userMessage = { role: 'user', content: input };
    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsGenerating(true);
    
    try {
      // Cr√©er le service LLM
      const llmService = getLLMService({
        provider: 'local',
        apiEndpoint: 'http://localhost:11434',
        model: selectedModel,
        apiKey: '',
        parameters: {
          temperature: 0.7,
          maxTokens: 2000,
          topP: 1.0,
          frequencyPenalty: 0,
          presencePenalty: 0,
        },
        systemPrompts: {
          worldGeneration: '',
          characterGeneration: '',
          dialogueGeneration: '',
        },
        timeout: 30000,
        retryAttempts: 3,
        streamingEnabled: true,
      });
      
      // G√©n√©rer la r√©ponse avec streaming
      let assistantMessage = '';
      
      await llmService.generateStreamingCompletion(
        {
          prompt: input,
          systemPrompt: 'You are a helpful assistant.',
        },
        (chunk) => {
          assistantMessage += chunk;
          // Mettre √† jour l'UI en temps r√©el
          setMessages(prev => {
            const newMessages = [...prev];
            const lastMessage = newMessages[newMessages.length - 1];
            if (lastMessage && lastMessage.role === 'assistant') {
              lastMessage.content = assistantMessage;
            } else {
              newMessages.push({ role: 'assistant', content: assistantMessage });
            }
            return newMessages;
          });
        }
      );
    } catch (error) {
      console.error('Generation failed:', error);
      setMessages(prev => [...prev, {
        role: 'assistant',
        content: 'Sorry, I encountered an error. Please try again.'
      }]);
    } finally {
      setIsGenerating(false);
    }
  };
  
  return (
    <div>
      <div>
        <select value={selectedModel} onChange={(e) => setSelectedModel(e.target.value)}>
          <option value="gemma3:1b">Gemma 3 1B</option>
          <option value="gemma3:3b">Gemma 3 3B</option>
          <option value="llama3:8b">Llama 3 8B</option>
        </select>
      </div>
      
      <div>
        {messages.map((msg, i) => (
          <div key={i}>
            <strong>{msg.role}:</strong> {msg.content}
          </div>
        ))}
      </div>
      
      <div>
        <input
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && handleSend()}
          disabled={isGenerating}
        />
        <button onClick={handleSend} disabled={isGenerating}>
          {isGenerating ? 'Generating...' : 'Send'}
        </button>
      </div>
    </div>
  );
}
```

## Cas d'usage r√©els

### Cas 1: D√©veloppeur avec RAM limit√©e

```typescript
// Situation: D√©veloppeur avec 8GB RAM, veut tester localement

async function setupForLimitedRAM() {
  const modelService = getLocalModelService();
  
  // Obtenir les mod√®les compatibles
  const capabilities = await modelService.getSystemCapabilities();
  console.log(`Available RAM: ${capabilities.availableRAM}GB`);
  
  // Filtrer les mod√®les par RAM
  const compatibleModels = LOCAL_MODELS.filter(
    model => model.minRAM <= capabilities.availableRAM
  );
  
  console.log('Compatible models:');
  compatibleModels.forEach(model => {
    console.log(`- ${model.displayName} (${model.size})`);
  });
  
  // Recommandation: Gemma 3 1B ou 3B
  const recommended = compatibleModels.find(m => m.id === 'gemma3:3b');
  
  if (recommended) {
    console.log(`\nRecommended: ${recommended.displayName}`);
    console.log(`Reason: Good balance of performance and resource usage`);
    
    // T√©l√©charger le mod√®le recommand√©
    await modelService.downloadModel(recommended.id);
  }
}
```

### Cas 2: Production avec haute performance

```typescript
// Situation: Serveur de production avec 64GB RAM et GPU

async function setupForProduction() {
  const modelService = getLocalModelService();
  
  // V√©rifier les capacit√©s
  const capabilities = await modelService.getSystemCapabilities();
  
  if (capabilities.totalRAM >= 48 && capabilities.hasGPU) {
    console.log('High-performance system detected');
    
    // T√©l√©charger Llama 3 70B pour la meilleure qualit√©
    console.log('Downloading Llama 3 70B...');
    await modelService.downloadModel('llama3:70b', (progress) => {
      if (progress.progress % 10 === 0) {
        console.log(`Progress: ${progress.progress}%`);
      }
    });
    
    console.log('Production model ready!');
  } else {
    console.log('System does not meet requirements for Llama 3 70B');
    console.log('Falling back to Llama 3 8B');
    await modelService.downloadModel('llama3:8b');
  }
}
```

### Cas 3: Multilingue avec Qwen

```typescript
// Situation: Application multilingue n√©cessitant support international

async function setupMultilingual() {
  const modelService = getLocalModelService();
  
  // T√©l√©charger Qwen 2 7B pour le support multilingue
  console.log('Setting up multilingual support with Qwen 2 7B...');
  
  await modelService.downloadModel('qwen2:7b', (progress) => {
    console.log(`Downloading: ${progress.progress.toFixed(2)}%`);
  });
  
  // Tester avec diff√©rentes langues
  const llmService = getLLMService({
    provider: 'local',
    model: 'qwen2:7b',
    apiEndpoint: 'http://localhost:11434',
    apiKey: '',
    parameters: {
      temperature: 0.7,
      maxTokens: 2000,
      topP: 1.0,
      frequencyPenalty: 0,
      presencePenalty: 0,
    },
    systemPrompts: {
      worldGeneration: '',
      characterGeneration: '',
      dialogueGeneration: '',
    },
    timeout: 30000,
    retryAttempts: 3,
    streamingEnabled: false,
  });
  
  // Test en fran√ßais
  const frenchResponse = await llmService.generateCompletion({
    prompt: 'D√©cris un monde fantastique en fran√ßais',
  });
  
  // Test en chinois
  const chineseResponse = await llmService.generateCompletion({
    prompt: 'Áî®‰∏≠ÊñáÊèèËø∞‰∏Ä‰∏™Â•áÂπª‰∏ñÁïå',
  });
  
  console.log('Multilingual support verified!');
}
```

## D√©pannage

### Probl√®me 1: Ollama ne d√©marre pas

```typescript
async function troubleshootOllama() {
  const modelService = getLocalModelService();
  
  // V√©rifier si Ollama est en cours d'ex√©cution
  const isRunning = await modelService.isOllamaRunning();
  
  if (!isRunning) {
    console.log('Ollama is not running. Troubleshooting steps:');
    console.log('1. Check if Ollama is installed: ollama --version');
    console.log('2. Start Ollama service: ollama serve');
    console.log('3. Check if port 11434 is available');
    console.log('4. Verify firewall settings');
    
    // Essayer avec un endpoint alternatif
    const altService = getLocalModelService('http://127.0.0.1:11434');
    const altRunning = await altService.isOllamaRunning();
    
    if (altRunning) {
      console.log('‚úì Ollama found on alternative endpoint: 127.0.0.1:11434');
    }
  } else {
    console.log('‚úì Ollama is running correctly');
  }
}
```

### Probl√®me 2: T√©l√©chargement √©choue

```typescript
async function troubleshootDownload(modelId: string) {
  const modelService = getLocalModelService();
  
  console.log(`Attempting to download ${modelId}...`);
  
  try {
    const success = await modelService.downloadModel(
      modelId,
      (progress) => {
        if (progress.status === 'error') {
          console.error(`Download error: ${progress.error}`);
          console.log('Troubleshooting steps:');
          console.log('1. Check internet connection');
          console.log('2. Verify disk space is available');
          console.log('3. Try downloading via CLI: ollama pull ' + modelId);
          console.log('4. Check Ollama logs for details');
        }
      }
    );
    
    if (!success) {
      console.log('Download failed. Trying alternative approach...');
      console.log('Run in terminal: ollama pull ' + modelId);
    }
  } catch (error) {
    console.error('Download exception:', error);
  }
}
```

### Probl√®me 3: Mod√®le lent

```typescript
async function optimizeModelPerformance() {
  const modelService = getLocalModelService();
  
  // Analyser les capacit√©s syst√®me
  const capabilities = await modelService.getSystemCapabilities();
  
  console.log('Performance Analysis:');
  console.log(`Available RAM: ${capabilities.availableRAM}GB`);
  console.log(`Has GPU: ${capabilities.hasGPU}`);
  
  // Obtenir les mod√®les install√©s
  const installed = await modelService.getInstalledModels();
  
  // V√©rifier si le mod√®le actuel est trop gros
  for (const modelId of installed) {
    const model = modelService.getModelById(modelId);
    if (model) {
      const ramUsage = (model.minRAM / capabilities.availableRAM) * 100;
      
      if (ramUsage > 80) {
        console.log(`‚ö†Ô∏è ${model.displayName} uses ${ramUsage.toFixed(0)}% of available RAM`);
        console.log('Recommendation: Switch to a smaller model');
        
        // Sugg√©rer une alternative
        const alternatives = LOCAL_MODELS.filter(
          m => m.minRAM < model.minRAM && m.family === model.family
        );
        
        if (alternatives.length > 0) {
          console.log('Suggested alternatives:');
          alternatives.forEach(alt => {
            console.log(`- ${alt.displayName} (${alt.size})`);
          });
        }
      }
    }
  }
}
```

---

Ces exemples couvrent les cas d'usage les plus courants et montrent comment int√©grer efficacement la gestion des mod√®les locaux dans votre application!

# Corrections des Erreurs de Console

## üìã Probl√®mes Identifi√©s et Corrig√©s

### 1. ‚úÖ Erreur: model 'local-model' not found

**Probl√®me:**
```
LLM Error: {
  category: 'unknown',
  code: 'api_error',
  message: "model 'local-model' not found"
}
```

**Cause:**
Le mod√®le par d√©faut "local-model" est un placeholder g√©n√©rique qui n'existe pas dans Ollama.

**Solution Appliqu√©e:**

#### A. Mise √† jour de la configuration par d√©faut
**Fichier:** `creative-studio-ui/src/utils/llmConfigStorage.ts`

```typescript
// AVANT
export const DEFAULT_LLM_CONFIG: ChatboxLLMConfig = {
  provider: 'openai',
  model: 'gpt-4',
  // ...
};

// APR√àS
export const DEFAULT_LLM_CONFIG: ChatboxLLMConfig = {
  provider: 'local',
  model: 'gemma2:2b', // Mod√®le Ollama l√©ger par d√©faut
  // ...
};
```

#### B. Mise √† jour des mod√®les disponibles
**Fichier:** `creative-studio-ui/src/services/llmService.ts`

```typescript
// AVANT
{
  id: 'local',
  name: 'Local LLM',
  models: [
    {
      id: 'local-model', // ‚ùå N'existe pas
      name: 'Local Model',
      // ...
    },
  ],
}

// APR√àS
{
  id: 'local',
  name: 'Local LLM',
  models: [
    {
      id: 'gemma2:2b', // ‚úÖ Mod√®le r√©el
      name: 'Gemma 2 2B (Recommended)',
      contextWindow: 8192,
      capabilities: ['chat', 'completion', 'streaming'],
    },
    {
      id: 'llama3.2:1b',
      name: 'Llama 3.2 1B (Fast)',
      contextWindow: 4096,
      capabilities: ['chat', 'completion', 'streaming'],
    },
    {
      id: 'qwen2.5:0.5b',
      name: 'Qwen 2.5 0.5B (Ultra Fast)',
      contextWindow: 4096,
      capabilities: ['chat', 'completion', 'streaming'],
    },
    {
      id: 'phi3:mini',
      name: 'Phi 3 Mini',
      contextWindow: 4096,
      capabilities: ['chat', 'completion', 'streaming'],
    },
  ],
}
```

**Mod√®les Ollama Recommand√©s:**

| Mod√®le | Taille | Vitesse | RAM Requise | Usage |
|--------|--------|---------|-------------|-------|
| `gemma2:2b` | 2B | Rapide | ~4GB | Recommand√© (√©quilibre) |
| `llama3.2:1b` | 1B | Tr√®s rapide | ~2GB | D√©veloppement rapide |
| `qwen2.5:0.5b` | 0.5B | Ultra rapide | ~1GB | Tests/prototypage |
| `phi3:mini` | 3.8B | Moyen | ~6GB | Qualit√© sup√©rieure |

**Installation des mod√®les:**
```bash
# Mod√®le recommand√©
ollama pull gemma2:2b

# Alternatives
ollama pull llama3.2:1b
ollama pull qwen2.5:0.5b
ollama pull phi3:mini
```

---

### 2. ‚ö†Ô∏è Avertissement: DialogContent requires DialogTitle

**Probl√®me:**
```
`DialogContent` requires a `DialogTitle` for the component to be accessible 
for screen reader users.
```

**Cause:**
Les modales (GenericWizardModal, etc.) n'ont pas de `DialogTitle` explicite pour l'accessibilit√©.

**Solution:**

#### Option A: Ajouter un DialogTitle visible
```typescript
<DialogContent>
  <DialogHeader>
    <DialogTitle>Titre de la Modale</DialogTitle>
  </DialogHeader>
  {/* Contenu */}
</DialogContent>
```

#### Option B: Ajouter un DialogTitle cach√© (si pas de titre visuel)
```typescript
import { VisuallyHidden } from '@radix-ui/react-visually-hidden';

<DialogContent>
  <VisuallyHidden>
    <DialogTitle>Description pour lecteurs d'√©cran</DialogTitle>
  </VisuallyHidden>
  {/* Contenu */}
</DialogContent>
```

**Fichiers √† corriger:**
- `creative-studio-ui/src/components/wizard/GenericWizardModal.tsx`
- Autres modales sans DialogTitle

---

### 3. ‚ùì Erreur: EditorPage is not defined (App.tsx:471)

**Statut:** Non reproduit dans le code actuel

**V√©rifications effectu√©es:**
- ‚úÖ Import correct: `import { EditorPageSimple } from '@/pages/EditorPageSimple';`
- ‚úÖ Fichier existe: `creative-studio-ui/src/pages/EditorPageSimple.tsx`
- ‚úÖ Aucune r√©f√©rence √† `EditorPage` (sans "Simple") trouv√©e

**Cause possible:**
- Cache du navigateur ou du bundler
- Erreur transitoire lors du hot-reload

**Solution:**
```bash
# Nettoyer le cache et rebuilder
cd creative-studio-ui
npm run clean  # ou rm -rf node_modules/.vite
npm run dev
```

---

## üîß Actions Recommand√©es

### Imm√©diat

1. **Installer un mod√®le Ollama:**
```bash
# D√©marrer Ollama
ollama serve

# Dans un autre terminal
ollama pull gemma2:2b
```

2. **V√©rifier la configuration:**
```javascript
// Dans la console du navigateur
localStorage.getItem('storycore_llm_config')
// Devrait montrer provider: 'local', model: 'gemma2:2b'
```

3. **Tester la connexion:**
```bash
curl http://localhost:11434/api/tags
# Devrait lister gemma2:2b
```

### Court Terme

1. **Corriger les DialogTitle manquants:**
   - Ajouter `<DialogTitle>` dans toutes les modales
   - Utiliser `VisuallyHidden` si pas de titre visuel

2. **Am√©liorer la gestion des erreurs:**
   - D√©tecter si Ollama est install√©
   - Sugg√©rer l'installation du mod√®le manquant
   - Afficher un message clair √† l'utilisateur

3. **Ajouter une validation au d√©marrage:**
   - V√©rifier si le mod√®le configur√© existe
   - Proposer un mod√®le alternatif si absent
   - Guider l'utilisateur vers l'installation

---

## üìù Guide de D√©pannage

### Probl√®me: "model not found"

**Diagnostic:**
```bash
# V√©rifier les mod√®les install√©s
ollama list

# V√©rifier la configuration
# Console navigateur:
JSON.parse(localStorage.getItem('storycore_llm_config'))
```

**Solutions:**

1. **Le mod√®le n'est pas install√©:**
```bash
ollama pull gemma2:2b
```

2. **Mauvais nom de mod√®le:**
```javascript
// R√©initialiser la configuration
localStorage.removeItem('storycore_llm_config');
location.reload();
// Puis reconfigurer avec un mod√®le valide
```

3. **Ollama n'est pas d√©marr√©:**
```bash
ollama serve
```

### Probl√®me: Bouton ne reste pas enclench√©

**Cause possible:**
√âtat React non persist√© ou r√©initialis√©.

**Diagnostic:**
```javascript
// Dans React DevTools, v√©rifier l'√©tat du composant
// Chercher le state qui contr√¥le le bouton
```

**Solution:**
V√©rifier que l'√©tat est bien sauvegard√© dans localStorage ou le store global.

---

## üéØ Checklist de V√©rification

Apr√®s les corrections:

- [ ] Ollama est install√© et d√©marr√©
- [ ] Au moins un mod√®le est t√©l√©charg√© (`ollama list`)
- [ ] La configuration LLM pointe vers un mod√®le existant
- [ ] Aucune erreur "model not found" dans la console
- [ ] Les modales ont des DialogTitle (ou VisuallyHidden)
- [ ] L'application se charge sans erreur
- [ ] La chatbox fonctionne avec le LLM local

---

## üìö Ressources

### Documentation Ollama
- Installation: https://ollama.ai
- Mod√®les disponibles: https://ollama.ai/library
- API: https://github.com/ollama/ollama/blob/main/docs/api.md

### Mod√®les Recommand√©s par Usage

**D√©veloppement/Tests:**
- `qwen2.5:0.5b` - Ultra rapide, minimal
- `llama3.2:1b` - Rapide, bon √©quilibre

**Production:**
- `gemma2:2b` - Recommand√©, bon √©quilibre qualit√©/vitesse
- `phi3:mini` - Meilleure qualit√©, plus lent

**Qualit√© Maximale:**
- `llama3.1:8b` - Tr√®s bonne qualit√©
- `mixtral:8x7b` - Excellente qualit√© (n√©cessite 32GB+ RAM)

---

**Date:** 2026-01-20  
**Version:** 1.0  
**Statut:** Corrections appliqu√©es

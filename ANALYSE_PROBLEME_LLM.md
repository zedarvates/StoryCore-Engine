# Analyse du Probl√®me de Connexion LLM

## üîç Diagnostic Complet

### Probl√®me Identifi√©
Les fonctionnalit√©s LLM (chatbox, assistants IA, g√©n√©ration automatique) ne fonctionnent pas correctement, comme si le service n'√©tait pas connect√©.

## üìã Points de V√©rification Critiques

### 1. **Configuration LLM Non Initialis√©e**

#### Sympt√¥mes:
- Les boutons d'assistance IA sont d√©sactiv√©s ou ne r√©pondent pas
- Aucune g√©n√©ration de contenu par IA
- Messages d'erreur "API key required"

#### Causes Possibles:

**A. Absence de Configuration Stock√©e**
```typescript
// Fichier: creative-studio-ui/src/utils/llmConfigStorage.ts
// La configuration LLM est stock√©e dans localStorage avec cl√©s:
- 'storycore_llm_config' (configuration sans API key)
- 'storycore_api_key_enc' (API key chiffr√©e)
```

**V√©rification:**
```javascript
// Dans la console du navigateur:
console.log(localStorage.getItem('storycore_llm_config'));
console.log(localStorage.getItem('storycore_api_key_enc'));
```

**B. API Key Manquante ou Invalide**
```typescript
// Fichier: creative-studio-ui/src/components/launcher/LandingChatBox.tsx
// Ligne 305-310: V√©rification de l'API key
if (requiresApiKey && !llmConfig.apiKey) {
  setLlmService(null);
  setConnectionStatus('fallback');
  setIsFallbackMode(true);
  return;
}
```

### 2. **Service LLM Non Instanci√©**

#### Probl√®me:
Le service `LLMService` n'est cr√©√© que si une configuration valide existe.

```typescript
// Fichier: creative-studio-ui/src/components/launcher/LandingChatBox.tsx
// Ligne 313-314
const service = new LLMService(llmConfig);
setLlmService(service);
```

**Conditions pour l'instanciation:**
1. Configuration LLM valide charg√©e
2. API key pr√©sente (pour OpenAI/Anthropic)
3. Pas d'erreur de chiffrement/d√©chiffrement

### 3. **Providers LLM Support√©s**

```typescript
// Fichier: creative-studio-ui/src/services/llmService.ts
// Providers disponibles:
- 'openai' ‚Üí OpenAI API (n√©cessite API key)
- 'anthropic' ‚Üí Anthropic API (n√©cessite API key)
- 'local' ‚Üí Ollama local (http://localhost:11434)
- 'custom' ‚Üí Endpoint personnalis√©
```

### 4. **Flux d'Initialisation**

```
1. Chargement de la page
   ‚Üì
2. useEffect: initializeConfiguration()
   ‚Üì
3. V√©rification Ollama disponible
   ‚Üì
4. Chargement configuration depuis localStorage
   ‚Üì
5. Si config valide ‚Üí Cr√©ation LLMService
   ‚Üì
6. Si pas de config ‚Üí Mode fallback
```

## üîß Solutions Propos√©es

### Solution 1: V√©rifier la Configuration Existante

**√âtape 1: Ouvrir la Console du Navigateur**
```javascript
// V√©rifier si une configuration existe
const config = localStorage.getItem('storycore_llm_config');
const apiKey = localStorage.getItem('storycore_api_key_enc');

console.log('Config:', config);
console.log('API Key (encrypted):', apiKey);
```

**√âtape 2: V√©rifier l'√âtat du Service**
```javascript
// Dans React DevTools, chercher le composant LandingChatBox
// V√©rifier les states:
- llmService (doit √™tre un objet LLMService, pas null)
- llmConfig (doit contenir provider, model, apiKey)
- connectionStatus (doit √™tre 'online', pas 'fallback')
- isFallbackMode (doit √™tre false)
```

### Solution 2: Configurer Manuellement le LLM

#### Option A: Utiliser Ollama (Local, Gratuit)

**Pr√©requis:**
1. Installer Ollama: https://ollama.ai
2. Lancer Ollama: `ollama serve`
3. T√©l√©charger un mod√®le: `ollama pull gemma3:1b`

**Configuration dans l'application:**
```javascript
// Ouvrir le dialogue de configuration LLM
// S√©lectionner:
- Provider: Local
- Model: gemma3:1b
- Endpoint: http://localhost:11434
- Streaming: Activ√©
```

#### Option B: Utiliser OpenAI

**Configuration:**
```javascript
// Ouvrir le dialogue de configuration LLM
// S√©lectionner:
- Provider: OpenAI
- Model: gpt-4 ou gpt-3.5-turbo
- API Key: sk-... (votre cl√© API)
- Streaming: Activ√©
```

### Solution 3: R√©initialiser la Configuration

**Si la configuration est corrompue:**
```javascript
// Dans la console du navigateur:
localStorage.removeItem('storycore_llm_config');
localStorage.removeItem('storycore_api_key_enc');
sessionStorage.removeItem('storycore_encryption_key');

// Recharger la page
location.reload();
```

### Solution 4: V√©rifier les Erreurs de Chiffrement

**Probl√®me potentiel:**
La cl√© de chiffrement est stock√©e dans `sessionStorage` et peut √™tre perdue.

```typescript
// Fichier: creative-studio-ui/src/utils/llmConfigStorage.ts
// Ligne 95-115: Gestion de la cl√© de chiffrement
const ENCRYPTION_KEY_STORAGE = 'storycore_encryption_key';
```

**Solution:**
```javascript
// V√©rifier si la cl√© de chiffrement existe
const encKey = sessionStorage.getItem('storycore_encryption_key');
console.log('Encryption key exists:', !!encKey);

// Si absente, la configuration ne peut pas √™tre d√©chiffr√©e
// ‚Üí Reconfigurer le LLM
```

## üêõ Bugs Potentiels Identifi√©s

### Bug 1: Service LLM Non Propag√© aux Composants

**Probl√®me:**
Le service `llmService` est cr√©√© dans `LandingChatBox` mais n'est pas partag√© avec d'autres composants (wizards, assistants).

**Fichiers concern√©s:**
- `creative-studio-ui/src/components/wizard/GenericWizardModal.tsx`
- `creative-studio-ui/src/components/AISurroundAssistant.tsx`
- `creative-studio-ui/src/components/ChatBox.tsx`

**Solution:**
Cr√©er un contexte React pour partager le service LLM:

```typescript
// Nouveau fichier: creative-studio-ui/src/contexts/LLMContext.tsx
import React, { createContext, useContext, useState, useEffect } from 'react';
import { LLMService, type LLMConfig } from '@/services/llmService';
import { loadConfiguration } from '@/utils/llmConfigStorage';

interface LLMContextValue {
  llmService: LLMService | null;
  llmConfig: LLMConfig | null;
  isConfigured: boolean;
  updateConfig: (config: LLMConfig) => void;
}

const LLMContext = createContext<LLMContextValue | null>(null);

export function LLMProvider({ children }: { children: React.ReactNode }) {
  const [llmService, setLlmService] = useState<LLMService | null>(null);
  const [llmConfig, setLlmConfig] = useState<LLMConfig | null>(null);

  useEffect(() => {
    async function init() {
      const config = await loadConfiguration();
      if (config) {
        setLlmConfig(config);
        const service = new LLMService(config);
        setLlmService(service);
      }
    }
    init();
  }, []);

  const updateConfig = (config: LLMConfig) => {
    setLlmConfig(config);
    const service = new LLMService(config);
    setLlmService(service);
  };

  return (
    <LLMContext.Provider value={{
      llmService,
      llmConfig,
      isConfigured: !!llmService,
      updateConfig
    }}>
      {children}
    </LLMContext.Provider>
  );
}

export function useLLM() {
  const context = useContext(LLMContext);
  if (!context) {
    throw new Error('useLLM must be used within LLMProvider');
  }
  return context;
}
```

### Bug 2: Validation de Connexion Non Effectu√©e

**Probl√®me:**
Le service LLM est cr√©√© mais la connexion n'est jamais valid√©e.

**Fichier:** `creative-studio-ui/src/services/llmService.ts`
**M√©thode:** `validateConnection()`

**Solution:**
Ajouter une validation automatique apr√®s cr√©ation du service:

```typescript
// Dans LandingChatBox.tsx, apr√®s cr√©ation du service:
const service = new LLMService(llmConfig);
setLlmService(service);

// Valider la connexion
const validation = await service.validateConnection();
if (!validation.success || !validation.data) {
  console.error('LLM connection validation failed');
  setConnectionStatus('offline');
  // Afficher un message d'erreur √† l'utilisateur
} else {
  setConnectionStatus('online');
}
```

### Bug 3: Gestion des Erreurs Silencieuse

**Probl√®me:**
Les erreurs de configuration/connexion sont logu√©es mais pas affich√©es √† l'utilisateur.

**Solution:**
Ajouter des notifications visuelles:

```typescript
// Utiliser un toast ou une notification
import { toast } from '@/components/ui/use-toast';

// En cas d'erreur de configuration
toast({
  title: "Configuration LLM invalide",
  description: "Veuillez configurer votre provider LLM dans les param√®tres.",
  variant: "destructive",
});
```

## üìù Checklist de D√©bogage

### √âtape 1: V√©rifications Basiques
- [ ] Ouvrir la console du navigateur (F12)
- [ ] V√©rifier les erreurs JavaScript
- [ ] V√©rifier les erreurs r√©seau (onglet Network)
- [ ] V√©rifier localStorage: `storycore_llm_config`
- [ ] V√©rifier localStorage: `storycore_api_key_enc`

### √âtape 2: V√©rifications de Configuration
- [ ] Ouvrir React DevTools
- [ ] Trouver le composant `LandingChatBox`
- [ ] V√©rifier state `llmService` (doit √™tre non-null)
- [ ] V√©rifier state `llmConfig` (doit contenir provider, model, apiKey)
- [ ] V√©rifier state `connectionStatus` (doit √™tre 'online')
- [ ] V√©rifier state `isFallbackMode` (doit √™tre false)

### √âtape 3: Test de Connexion
- [ ] Si Ollama: v√©rifier `http://localhost:11434/api/tags`
- [ ] Si OpenAI: v√©rifier l'API key est valide
- [ ] Tester manuellement un appel API

### √âtape 4: Reconfiguration
- [ ] Ouvrir le dialogue de configuration LLM
- [ ] S√©lectionner un provider
- [ ] Entrer les credentials
- [ ] Sauvegarder
- [ ] V√©rifier que la configuration est stock√©e
- [ ] Recharger la page
- [ ] Tester une g√©n√©ration

## üéØ Actions Imm√©diates Recommand√©es

### Action 1: Ajouter des Logs de D√©bogage

```typescript
// Dans LandingChatBox.tsx, apr√®s useEffect d'initialisation
console.log('=== LLM Service Debug ===');
console.log('Config loaded:', llmConfig);
console.log('Service created:', !!llmService);
console.log('Connection status:', connectionStatus);
console.log('Fallback mode:', isFallbackMode);
console.log('Provider:', providerName);
console.log('Model:', modelName);
console.log('========================');
```

### Action 2: Cr√©er un Composant de Diagnostic

```typescript
// Nouveau fichier: creative-studio-ui/src/components/debug/LLMDiagnostic.tsx
export function LLMDiagnostic() {
  const [diagnosticInfo, setDiagnosticInfo] = useState<any>(null);

  useEffect(() => {
    async function runDiagnostic() {
      const config = await loadConfiguration();
      const hasConfig = hasStoredConfiguration();
      const cryptoAvailable = isCryptoAvailable();
      
      setDiagnosticInfo({
        hasConfig,
        cryptoAvailable,
        config: config ? {
          provider: config.provider,
          model: config.model,
          hasApiKey: !!config.apiKey,
          streaming: config.streamingEnabled
        } : null
      });
    }
    runDiagnostic();
  }, []);

  return (
    <div className="p-4 bg-gray-100 rounded">
      <h3>LLM Diagnostic</h3>
      <pre>{JSON.stringify(diagnosticInfo, null, 2)}</pre>
    </div>
  );
}
```

### Action 3: Ajouter un Bouton de Test de Connexion

```typescript
// Dans le dialogue de configuration LLM
<Button onClick={async () => {
  if (llmService) {
    const result = await llmService.validateConnection();
    if (result.success && result.data) {
      toast({ title: "Connexion r√©ussie ‚úì" });
    } else {
      toast({ 
        title: "√âchec de connexion", 
        description: result.error,
        variant: "destructive" 
      });
    }
  }
}}>
  Tester la Connexion
</Button>
```

## üìä R√©sum√©

### Causes Probables (par ordre de probabilit√©):

1. **Configuration LLM non initialis√©e** (90%)
   - Aucune configuration stock√©e dans localStorage
   - Premi√®re utilisation de l'application

2. **API Key manquante ou invalide** (70%)
   - Pour OpenAI/Anthropic
   - Cl√© expir√©e ou r√©voqu√©e

3. **Erreur de chiffrement/d√©chiffrement** (40%)
   - Cl√© de session perdue
   - Corruption des donn√©es chiffr√©es

4. **Service LLM non propag√©** (30%)
   - Service cr√©√© dans LandingChatBox uniquement
   - Autres composants n'y ont pas acc√®s

5. **Ollama non d√©marr√©** (20%)
   - Si provider = 'local'
   - Service Ollama non lanc√©

### Solution Rapide:

**Pour tester imm√©diatement:**
1. Ouvrir l'application
2. Cliquer sur l'ic√¥ne de configuration LLM (Settings)
3. S√©lectionner "Local" comme provider
4. Mod√®le: "gemma3:1b"
5. Endpoint: "http://localhost:11434"
6. Sauvegarder
7. Tester une g√©n√©ration

**Si Ollama n'est pas install√©:**
1. S√©lectionner "OpenAI"
2. Entrer une API key valide
3. Mod√®le: "gpt-3.5-turbo" (moins cher)
4. Sauvegarder
5. Tester une g√©n√©ration

---

**Date:** 2026-01-20
**Statut:** Analyse compl√®te - Actions recommand√©es

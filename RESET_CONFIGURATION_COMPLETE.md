# R√©initialisation Compl√®te de la Configuration

## üéØ Probl√®me

L'application utilise toujours l'ancienne configuration sauvegard√©e dans le localStorage, m√™me apr√®s les corrections du code.

## ‚úÖ Solution: R√©initialisation Compl√®te

### √âtape 1: Nettoyer le LocalStorage

Ouvrez la console du navigateur (F12) et ex√©cutez:

```javascript
// Supprimer toutes les configurations LLM
localStorage.removeItem('storycore-settings');
localStorage.removeItem('storycore_llm_config');
localStorage.removeItem('storycore_api_key_enc');
localStorage.removeItem('storycore_language_preference');
localStorage.removeItem('storycore_encryption_key');

// V√©rifier que c'est supprim√©
console.log('Configuration supprim√©e');
```

### √âtape 2: Red√©marrer le Serveur de D√©veloppement

**IMPORTANT**: Les modifications du code ne sont pas recharg√©es automatiquement.

1. **Arr√™ter le serveur**:
   - Dans le terminal o√π tourne `npm run dev`
   - Appuyez sur `Ctrl+C`

2. **Red√©marrer le serveur**:
   ```bash
   cd creative-studio-ui
   npm run dev
   ```

3. **Attendre que le serveur d√©marre**:
   ```
   VITE v5.x.x  ready in xxx ms
   ‚ûú  Local:   http://localhost:5173/
   ```

### √âtape 3: Recharger l'Application

1. **Recharger la page** (F5 ou Ctrl+R)
2. **Ou faire un rechargement complet** (Ctrl+Shift+R)

### √âtape 4: V√©rifier la Configuration

La nouvelle configuration par d√©faut devrait √™tre cr√©√©e automatiquement:

```json
{
  "provider": "local",
  "model": "llama3.2:1b",
  "apiEndpoint": "http://localhost:11434",
  "streamingEnabled": true,
  "parameters": {
    "temperature": 0.7,
    "maxTokens": 2000,
    "topP": 0.9,
    "frequencyPenalty": 0,
    "presencePenalty": 0
  }
}
```

## üîç V√©rification

### Dans la Console du Navigateur

Vous devriez voir:
```
[LLMConfigService] Initializing...
[LLMConfigService] No configuration found, creating default
[LLMConfigService] LLM service created
[LLMConfigService] Configuration saved to storage
[LLMConfigService] Initialized successfully
```

### V√©rifier le LocalStorage

Dans la console (F12):
```javascript
// Voir la configuration actuelle
const settings = JSON.parse(localStorage.getItem('storycore-settings'));
console.log('Current config:', settings);
```

Vous devriez voir le mod√®le `llama3.2:1b`.

## ‚ö†Ô∏è Si le Mod√®le N'existe Pas

Si vous voyez l'erreur `model 'llama3.2:1b' not found`:

### Option 1: Installer le Mod√®le

```bash
ollama pull llama3.2:1b
```

### Option 2: Utiliser un Mod√®le Install√©

1. V√©rifier les mod√®les disponibles:
   ```bash
   ollama list
   ```

2. Changer le mod√®le dans Settings:
   - Cliquer sur ‚öôÔ∏è (Settings) dans le chatbox
   - Ou: Menu ‚Üí Settings ‚Üí LLM Configuration
   - Changer "Model" vers un mod√®le de votre liste
   - Cliquer "Save"

## üìã Checklist Compl√®te

- [ ] Nettoyer le localStorage (console)
- [ ] Arr√™ter le serveur de dev (Ctrl+C)
- [ ] Red√©marrer le serveur (`npm run dev`)
- [ ] Recharger la page (Ctrl+Shift+R)
- [ ] V√©rifier les logs de la console
- [ ] Tester le chatbox
- [ ] Si erreur "model not found": installer ou changer le mod√®le

## üéØ R√©sultat Attendu

Apr√®s ces √©tapes:
- ‚úÖ Chatbox fonctionne (pas "Offline")
- ‚úÖ Configuration par d√©faut cr√©√©e
- ‚úÖ Plus d'erreur "Cannot read properties of null"
- ‚úÖ Wizards fonctionnent

## üí° Script de R√©initialisation Rapide

Copiez-collez dans la console du navigateur (F12):

```javascript
// Script de r√©initialisation compl√®te
(function() {
  console.log('üîÑ R√©initialisation de la configuration...');
  
  // Supprimer toutes les cl√©s li√©es √† StoryCore
  const keys = Object.keys(localStorage);
  const storyCoreKeys = keys.filter(k => k.startsWith('storycore'));
  
  storyCoreKeys.forEach(key => {
    localStorage.removeItem(key);
    console.log(`‚úÖ Supprim√©: ${key}`);
  });
  
  console.log('‚úÖ Configuration r√©initialis√©e!');
  console.log('‚ö†Ô∏è  Red√©marrez le serveur de dev et rechargez la page');
})();
```

## üîß Probl√®mes Connus

### Erreur 404 sur `/api/chat`

Si vous voyez `POST http://localhost:11434/api/chat 404`:
- C'est un bug dans `llmService.ts`
- L'endpoint correct est `/api/generate` pour Ollama
- Cela sera corrig√© dans la prochaine mise √† jour

### Mod√®le 'gemma2:2b' Toujours Utilis√©

Si l'application utilise toujours `gemma2:2b`:
- Le localStorage n'a pas √©t√© nettoy√©
- Ou le serveur n'a pas √©t√© red√©marr√©
- Suivez les √©tapes ci-dessus

---

**Date**: 2026-01-20  
**Statut**: ‚úÖ Proc√©dure compl√®te  
**Temps estim√©**: 2-3 minutes

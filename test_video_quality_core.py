"""
Core Functionality Test for Advanced Video Quality Monitor

Tests the core functionality without OpenCV dependencies
to validate the implementation architecture.

Author: StoryCore-Engine Team
Version: 1.0.0
"""

import sys
import time
from pathlib import Path
from unittest.mock import Mock, patch
import numpy as np

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

# Mock cv2 completely
sys.modules['cv2'] = Mock()

try:
    from advanced_video_quality_monitor import (
        AdvancedVideoQualityMonitor,
        QualityConfig,
        QualityThresholds,
        QualityMetric,
        QualitySeverity,
        ImprovementStrategy,
        QualityIssue,
        QualityReport,
        create_quality_monitor
    )
    print("‚úÖ Successfully imported Advanced Video Quality Monitor")
except ImportError as e:
    print(f"‚ùå Import failed: {e}")
    sys.exit(1)

def test_core_architecture():
    """Test core architecture and data structures."""
    print("\nüèóÔ∏è Testing Core Architecture")
    
    # Test 1: Enum definitions
    metrics = list(QualityMetric)
    severities = list(QualitySeverity)
    strategies = list(ImprovementStrategy)
    
    print(f"‚úÖ Quality metrics defined: {len(metrics)} types")
    print(f"‚úÖ Severity levels defined: {len(severities)} levels")
    print(f"‚úÖ Improvement strategies defined: {len(strategies)} strategies")
    
    # Test 2: Data class creation
    issue = QualityIssue(
        metric=QualityMetric.VISUAL_QUALITY,
        severity=QualitySeverity.HIGH,
        score=0.6,
        threshold=0.8,
        description="Test issue"
    )
    print(f"‚úÖ QualityIssue created: {issue.metric.value}")
    
    # Test 3: Configuration system
    thresholds = QualityThresholds(
        temporal_consistency=0.80,
        visual_quality=0.85
    )
    config = QualityConfig(thresholds=thresholds)
    print(f"‚úÖ Configuration system: {config.thresholds.temporal_consistency}")
    
    # Test 4: Monitor initialization
    monitor = AdvancedVideoQualityMonitor(config)
    print(f"‚úÖ Monitor initialized: {monitor.config is not None}")
    
    return {
        "metrics_count": len(metrics),
        "severities_count": len(severities),
        "strategies_count": len(strategies),
        "issue_created": issue.score == 0.6,
        "config_created": config.thresholds.temporal_consistency == 0.80,
        "monitor_created": monitor.config is not None
    }

def test_quality_calculations():
    """Test quality calculation methods without OpenCV."""
    print("\nüßÆ Testing Quality Calculations")
    
    monitor = create_quality_monitor()
    
    # Test 1: Overall score calculation
    metric_scores = {
        QualityMetric.TEMPORAL_CONSISTENCY: 0.8,
        QualityMetric.MOTION_SMOOTHNESS: 0.7,
        QualityMetric.VISUAL_QUALITY: 0.9,
        QualityMetric.ARTIFACT_DETECTION: 0.85
    }
    
    overall_score = monitor._calculate_overall_score(metric_scores)
    print(f"‚úÖ Overall score calculation: {overall_score:.3f}")
    
    # Test 2: Score to grade conversion
    test_scores = [0.95, 0.85, 0.75, 0.65, 0.45]
    expected_grades = ["A", "B", "C", "D", "F"]
    
    grades_correct = True
    for score, expected in zip(test_scores, expected_grades):
        grade = monitor._score_to_grade(score)
        if grade != expected:
            grades_correct = False
        print(f"   Score {score} ‚Üí Grade {grade}")
    
    print(f"‚úÖ Grade conversion: {grades_correct}")
    
    # Test 3: Threshold checking
    thresholds_work = True
    for metric in QualityMetric:
        threshold = monitor._get_threshold(metric)
        if not (0.0 <= threshold <= 1.0):
            thresholds_work = False
    
    print(f"‚úÖ Threshold validation: {thresholds_work}")
    
    # Test 4: Improvement suggestions
    mock_report = QualityReport(
        video_path="test.mp4",
        overall_score=0.5,
        metric_scores={QualityMetric.VISUAL_QUALITY: 0.6},
        issues=[QualityIssue(
            metric=QualityMetric.VISUAL_QUALITY,
            severity=QualitySeverity.HIGH,
            score=0.6,
            threshold=0.8,
            description="Low quality"
        )],
        improvement_suggestions=[],
        analysis_time=1.0,
        frame_count=30,
        resolution=(720, 1280),
        duration=1.0,
        workflow_type="test"
    )
    
    suggestions = monitor._generate_improvement_suggestions(mock_report)
    suggestions_generated = len(suggestions) > 0
    print(f"‚úÖ Improvement suggestions: {len(suggestions)} generated")
    
    return {
        "overall_score_valid": 0.0 <= overall_score <= 1.0,
        "grades_correct": grades_correct,
        "thresholds_work": thresholds_work,
        "suggestions_generated": suggestions_generated,
        "overall_score": overall_score
    }

def test_configuration_system():
    """Test configuration system flexibility."""
    print("\n‚öôÔ∏è Testing Configuration System")
    
    # Test 1: Default configuration
    default_config = QualityConfig()
    print(f"‚úÖ Default config: real-time={default_config.enable_real_time}")
    
    # Test 2: Custom thresholds
    custom_thresholds = QualityThresholds(
        temporal_consistency=0.90,
        visual_quality=0.95,
        motion_smoothness=0.85
    )
    custom_config = QualityConfig(
        thresholds=custom_thresholds,
        enable_alpha_analysis=True,
        sample_frame_rate=0.5
    )
    
    monitor = AdvancedVideoQualityMonitor(custom_config)
    print(f"‚úÖ Custom config: alpha={custom_config.enable_alpha_analysis}")
    
    # Test 3: Configuration validation
    extreme_config = QualityConfig(
        thresholds=QualityThresholds(
            temporal_consistency=1.5,  # Invalid
            visual_quality=-0.1        # Invalid
        )
    )
    
    # Should handle gracefully
    extreme_monitor = AdvancedVideoQualityMonitor(extreme_config)
    config_handled = extreme_monitor.config is not None
    print(f"‚úÖ Extreme config handled: {config_handled}")
    
    return {
        "default_config": default_config.enable_real_time,
        "custom_config": custom_config.enable_alpha_analysis,
        "extreme_handled": config_handled
    }

def test_error_handling():
    """Test error handling and resilience."""
    print("\nüõ°Ô∏è Testing Error Handling")
    
    monitor = create_quality_monitor()
    
    # Test 1: Empty frames list
    with patch.object(monitor, '_load_video_frames', return_value=[]):
        report = monitor.analyze_video("empty.mp4", "test")
        empty_handled = report.overall_score == 0.0
        print(f"‚úÖ Empty frames handled: {empty_handled}")
    
    # Test 2: Exception during analysis
    def mock_exception(*args, **kwargs):
        raise ValueError("Test exception")
    
    with patch.object(monitor, '_load_video_frames', side_effect=mock_exception):
        report = monitor.analyze_video("error.mp4", "test")
        exception_handled = len(report.issues) > 0 and report.overall_score == 0.0
        print(f"‚úÖ Exception handled: {exception_handled}")
    
    # Test 3: Invalid video path
    report = monitor.analyze_video("", "test")
    invalid_path_handled = report.overall_score >= 0.0
    print(f"‚úÖ Invalid path handled: {invalid_path_handled}")
    
    return {
        "empty_handled": empty_handled,
        "exception_handled": exception_handled,
        "invalid_path_handled": invalid_path_handled
    }

def test_dashboard_integration():
    """Test dashboard data generation."""
    print("\nüìä Testing Dashboard Integration")
    
    monitor = create_quality_monitor()
    
    # Create mock analysis result
    mock_report = QualityReport(
        video_path="dashboard_test.mp4",
        overall_score=0.82,
        metric_scores={
            QualityMetric.TEMPORAL_CONSISTENCY: 0.85,
            QualityMetric.VISUAL_QUALITY: 0.80,
            QualityMetric.MOTION_SMOOTHNESS: 0.78
        },
        issues=[],
        improvement_suggestions=["Test suggestion"],
        analysis_time=2.5,
        frame_count=60,
        resolution=(1080, 1920),
        duration=2.0,
        workflow_type="hunyuan_t2v"
    )
    
    # Cache the report
    monitor.analysis_cache["dashboard_test.mp4"] = mock_report
    
    # Test dashboard data generation
    dashboard_data = monitor.get_quality_dashboard_data("dashboard_test.mp4")
    
    required_keys = ["overall_score", "grade", "metrics", "issues", "suggestions", "stats"]
    has_all_keys = all(key in dashboard_data for key in required_keys)
    print(f"‚úÖ Dashboard data structure: {has_all_keys}")
    
    # Test grade assignment
    grade_valid = dashboard_data.get("grade") in ["A", "B", "C", "D", "F"]
    print(f"‚úÖ Grade assignment: {dashboard_data.get('grade')} (valid: {grade_valid})")
    
    # Test metrics formatting
    metrics_formatted = isinstance(dashboard_data.get("metrics"), dict)
    print(f"‚úÖ Metrics formatting: {metrics_formatted}")
    
    # Test non-existent video
    no_data = monitor.get_quality_dashboard_data("nonexistent.mp4")
    error_handled = "error" in no_data
    print(f"‚úÖ Missing data handled: {error_handled}")
    
    return {
        "has_all_keys": has_all_keys,
        "grade_valid": grade_valid,
        "metrics_formatted": metrics_formatted,
        "error_handled": error_handled,
        "overall_score": dashboard_data.get("overall_score", 0)
    }

def test_export_functionality():
    """Test report export functionality."""
    print("\nüì§ Testing Export Functionality")
    
    monitor = create_quality_monitor()
    
    # Create and cache a mock report
    mock_report = QualityReport(
        video_path="export_test.mp4",
        overall_score=0.75,
        metric_scores={QualityMetric.VISUAL_QUALITY: 0.75},
        issues=[],
        improvement_suggestions=["Export test"],
        analysis_time=1.0,
        frame_count=30,
        resolution=(720, 1280),
        duration=1.0,
        workflow_type="test"
    )
    
    monitor.analysis_cache["export_test.mp4"] = mock_report
    
    # Test export (without actually writing file)
    with patch("builtins.open", create=True) as mock_open:
        with patch("json.dump") as mock_json_dump:
            success = monitor.export_quality_report("export_test.mp4", "test_output.json")
            export_attempted = mock_open.called and mock_json_dump.called
            print(f"‚úÖ Export functionality: {export_attempted}")
    
    # Test export with missing data
    missing_success = monitor.export_quality_report("missing.mp4", "output.json")
    missing_handled = not missing_success
    print(f"‚úÖ Missing data export: {missing_handled}")
    
    return {
        "export_attempted": export_attempted,
        "missing_handled": missing_handled
    }

def main():
    """Run all core functionality tests."""
    print("üé¨ Advanced Video Quality Monitor - Core Functionality Test")
    print("=" * 70)
    
    try:
        # Run all test suites
        arch_results = test_core_architecture()
        calc_results = test_quality_calculations()
        config_results = test_configuration_system()
        error_results = test_error_handling()
        dashboard_results = test_dashboard_integration()
        export_results = test_export_functionality()
        
        # Calculate success metrics
        total_tests = 0
        passed_tests = 0
        
        for results in [arch_results, calc_results, config_results, 
                       error_results, dashboard_results, export_results]:
            for key, value in results.items():
                total_tests += 1
                if value:
                    passed_tests += 1
        
        success_rate = passed_tests / total_tests if total_tests > 0 else 0
        
        # Summary
        print("\n" + "=" * 70)
        print("üìä CORE FUNCTIONALITY TEST SUMMARY")
        print("=" * 70)
        
        print(f"Architecture Tests: {sum(arch_results.values())}/{len(arch_results)} passed")
        print(f"Calculation Tests: {sum(calc_results.values())}/{len(calc_results)} passed")
        print(f"Configuration Tests: {sum(config_results.values())}/{len(config_results)} passed")
        print(f"Error Handling Tests: {sum(error_results.values())}/{len(error_results)} passed")
        print(f"Dashboard Tests: {sum(dashboard_results.values())}/{len(dashboard_results)} passed")
        print(f"Export Tests: {sum(export_results.values())}/{len(export_results)} passed")
        
        print(f"\nüìà Overall Success Rate: {success_rate:.1%} ({passed_tests}/{total_tests})")
        
        # Assessment
        if success_rate >= 0.9:
            status = "‚úÖ EXCELLENT"
            message = "Core functionality is robust and ready for integration!"
        elif success_rate >= 0.8:
            status = "‚úÖ GOOD"
            message = "Core functionality is solid with minor areas for improvement."
        elif success_rate >= 0.7:
            status = "‚ö†Ô∏è ACCEPTABLE"
            message = "Core functionality works but needs attention in some areas."
        else:
            status = "‚ùå NEEDS WORK"
            message = "Core functionality has significant issues that need addressing."
        
        print(f"\nüéØ ASSESSMENT: {status}")
        print(f"   {message}")
        
        # Key achievements
        if success_rate >= 0.8:
            print("\nüöÄ KEY ACHIEVEMENTS:")
            print("   - Complete architecture with 10 quality metrics")
            print("   - Flexible configuration system with custom thresholds")
            print("   - Robust error handling and graceful degradation")
            print("   - Dashboard integration with grade system")
            print("   - Export functionality for detailed reports")
            print("   - Support for multiple workflow types")
        
        return success_rate >= 0.8
        
    except Exception as e:
        print(f"\n‚ùå Test execution failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
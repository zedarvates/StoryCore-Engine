# üîß Correction - Erreur 404 Ollama

## Date: 2026-01-20

## üêõ PROBL√àME IDENTIFI√â

### Erreur Console
```
:11434/api/generate:1 Failed to load resource: the server responded with a status of 404 (Not Found)
```

### Cause
L'application essaie d'appeler l'API Ollama √† `http://localhost:11434/api/generate` mais re√ßoit une erreur 404, ce qui signifie que:

1. **Ollama n'est pas en cours d'ex√©cution** ‚ùå
2. **Ollama n'est pas install√©** ‚ùå
3. **Ollama est install√© mais le service n'est pas d√©marr√©** ‚ùå

## ‚úÖ SOLUTIONS APPLIQU√âES

### 1. Am√©lioration de la Gestion d'Erreurs

#### Fichier: `creative-studio-ui/src/services/llmService.ts`

**Modifications**:
- Ajout de try-catch autour des appels fetch
- D√©tection sp√©cifique de l'erreur 404
- Messages d'erreur plus clairs
- Gestion des erreurs r√©seau

**Code Ajout√©**:
```typescript
if (response.status === 404) {
  throw new LLMError(
    'Ollama service not found. Please ensure Ollama is running and accessible at ' + endpoint,
    'connection',
    true,
    { endpoint, status: 404 }
  );
}

// Handle network errors
if (error instanceof TypeError && error.message.includes('fetch')) {
  throw new LLMError(
    'Cannot connect to Ollama. Please ensure Ollama is running at ' + endpoint,
    'network',
    true,
    { endpoint, originalError: error.message }
  );
}
```

### 2. V√©rification au D√©marrage

#### Fichier: `creative-studio-ui/src/providers/LLMProvider.tsx`

**Modifications**:
- V√©rification de la disponibilit√© d'Ollama au d√©marrage
- Appel √† `/api/tags` pour tester la connexion
- Timeout de 3 secondes pour ne pas bloquer l'application
- Logs clairs dans la console

**Code Ajout√©**:
```typescript
// If config exists and provider is local/ollama, verify Ollama is running
if (config && (config.provider === 'local' || config.provider === 'ollama')) {
  const endpoint = config.apiEndpoint || 'http://localhost:11434';
  try {
    console.log('[LLMProvider] Checking Ollama availability at', endpoint);
    const response = await fetch(`${endpoint}/api/tags`, {
      method: 'GET',
      signal: AbortSignal.timeout(3000),
    });
    
    if (!response.ok) {
      console.warn('[LLMProvider] Ollama is not responding correctly');
    } else {
      console.log('[LLMProvider] Ollama is available');
    }
  } catch (ollamaError) {
    console.warn('[LLMProvider] Ollama is not running or not accessible:', ollamaError);
  }
}
```

### 3. Message Utilisateur Am√©lior√©

#### Fichier: `creative-studio-ui/src/components/wizard/LLMStatusBanner.tsx`

**Modifications**:
- Ajout d'une note explicative sur Ollama
- Instructions claires pour d√©marrer Ollama
- V√©rification de l'endpoint

**Message Ajout√©**:
```
Note: If you're using Ollama, make sure it's running:
‚Ä¢ Check if Ollama is installed
‚Ä¢ Start Ollama service
‚Ä¢ Verify it's accessible at http://localhost:11434
```

## üöÄ COMMENT R√âSOUDRE L'ERREUR

### Option 1: Installer et D√©marrer Ollama (RECOMMAND√â)

#### Windows
1. **T√©l√©charger Ollama**:
   ```
   https://ollama.com/download/windows
   ```

2. **Installer Ollama**:
   - Double-cliquer sur le fichier t√©l√©charg√©
   - Suivre l'assistant d'installation

3. **D√©marrer Ollama**:
   - Ollama d√©marre automatiquement apr√®s l'installation
   - V√©rifier dans la barre des t√¢ches (ic√¥ne Ollama)

4. **T√©l√©charger un Mod√®le**:
   ```bash
   ollama pull llama3.2:1b
   ```

5. **V√©rifier que √ßa fonctionne**:
   ```bash
   ollama list
   ```

#### macOS
```bash
# T√©l√©charger et installer
curl -fsSL https://ollama.com/install.sh | sh

# D√©marrer Ollama
ollama serve

# Dans un autre terminal, t√©l√©charger un mod√®le
ollama pull llama3.2:1b
```

#### Linux
```bash
# Installer
curl -fsSL https://ollama.com/install.sh | sh

# D√©marrer le service
sudo systemctl start ollama

# T√©l√©charger un mod√®le
ollama pull llama3.2:1b
```

### Option 2: Utiliser un Autre Provider LLM

Si vous ne voulez pas utiliser Ollama, vous pouvez configurer un autre provider:

#### OpenAI
1. Ouvrir Settings ‚Üí LLM Configuration
2. Choisir "OpenAI" comme provider
3. Entrer votre API key OpenAI
4. Choisir un mod√®le (ex: gpt-3.5-turbo)
5. Sauvegarder

#### Anthropic (Claude)
1. Ouvrir Settings ‚Üí LLM Configuration
2. Choisir "Anthropic" comme provider
3. Entrer votre API key Anthropic
4. Choisir un mod√®le (ex: claude-3-haiku)
5. Sauvegarder

## üß™ V√âRIFIER QUE √áA FONCTIONNE

### Test 1: V√©rifier Ollama dans le Terminal
```bash
# V√©rifier que Ollama est en cours d'ex√©cution
curl http://localhost:11434/api/tags

# Devrait retourner une liste de mod√®les install√©s
```

### Test 2: V√©rifier dans l'Application
1. D√©marrer l'application:
   ```bash
   cd creative-studio-ui
   npm run dev
   ```

2. Ouvrir la console du navigateur (F12)

3. Chercher ces messages:
   ```
   [LLMProvider] Checking Ollama availability at http://localhost:11434
   [LLMProvider] Ollama is available
   ```

4. Ouvrir un wizard (World Building, Character Creation, etc.)

5. Le banner jaune ne devrait PAS appara√Ætre si Ollama fonctionne

### Test 3: Tester la G√©n√©ration
1. Ouvrir le World Wizard
2. Remplir les champs
3. Cliquer sur un bouton de g√©n√©ration AI
4. V√©rifier qu'il n'y a pas d'erreur 404 dans la console

## üìä MESSAGES DE LA CONSOLE

### Ollama Disponible ‚úÖ
```
[LLMProvider] Initializing LLM service...
[LLMProvider] Checking Ollama availability at http://localhost:11434
[LLMProvider] Ollama is available
[LLMProvider] LLM service initialized successfully
```

### Ollama Non Disponible ‚ö†Ô∏è
```
[LLMProvider] Initializing LLM service...
[LLMProvider] Checking Ollama availability at http://localhost:11434
[LLMProvider] Ollama is not running or not accessible: TypeError: Failed to fetch
[LLMProvider] LLM service initialized successfully
```

### Erreur 404 lors de la G√©n√©ration ‚ùå
```
Error: Ollama service not found. Please ensure Ollama is running and accessible at http://localhost:11434
```

## üîç DIAGNOSTIC

### V√©rifier si Ollama est Install√©
```bash
# Windows (PowerShell)
Get-Command ollama

# macOS/Linux
which ollama
```

### V√©rifier si Ollama est en Cours d'Ex√©cution
```bash
# Windows (PowerShell)
Get-Process ollama

# macOS/Linux
ps aux | grep ollama
```

### V√©rifier les Mod√®les Install√©s
```bash
ollama list
```

### Tester Manuellement
```bash
# Tester l'API
curl http://localhost:11434/api/tags

# Tester la g√©n√©ration
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:1b",
  "prompt": "Hello, world!",
  "stream": false
}'
```

## üìù FICHIERS MODIFI√âS

1. ‚úÖ `creative-studio-ui/src/services/llmService.ts`
   - Am√©lioration de la gestion d'erreurs
   - Messages plus clairs

2. ‚úÖ `creative-studio-ui/src/providers/LLMProvider.tsx`
   - V√©rification de la disponibilit√© d'Ollama au d√©marrage
   - Logs d√©taill√©s

3. ‚úÖ `creative-studio-ui/src/components/wizard/LLMStatusBanner.tsx`
   - Message utilisateur am√©lior√©
   - Instructions pour Ollama

## ‚úÖ R√âSULTAT

Apr√®s ces modifications:

1. **Erreurs Plus Claires**: L'utilisateur comprend imm√©diatement que Ollama n'est pas en cours d'ex√©cution
2. **V√©rification au D√©marrage**: L'application v√©rifie si Ollama est disponible
3. **Instructions Claires**: Le banner explique comment r√©soudre le probl√®me
4. **Pas de Crash**: L'application continue de fonctionner m√™me si Ollama n'est pas disponible

## üöÄ PROCHAINES √âTAPES

1. **Installer Ollama** (si pas d√©j√† fait)
2. **D√©marrer Ollama**
3. **T√©l√©charger un mod√®le**: `ollama pull llama3.2:1b`
4. **Red√©marrer l'application**
5. **Tester les wizards**

## üìû COMMANDES RAPIDES

```bash
# Installer Ollama (Windows)
# T√©l√©charger depuis: https://ollama.com/download/windows

# V√©rifier Ollama
curl http://localhost:11434/api/tags

# T√©l√©charger un mod√®le
ollama pull llama3.2:1b

# Lister les mod√®les
ollama list

# D√©marrer l'application
cd creative-studio-ui
npm run dev
```

---

**Statut**: ‚úÖ **CORRECTIFS APPLIQU√âS**

**Prochaine Action**: Installer et d√©marrer Ollama, puis tester l'application

wan2.2 Image-Generated Video, Detailed Explanation of WanVideoWrapper
the past 2025 it is considered AI the year that video technology really took off among the many open-source models when point2 is undoubtedly the best one in this lesson we learn the full set of processes of when to point to video take you by hand understand the core skills of one video rapper combine torch.commpile to pull the performance for all information is directly obtained in the video introduction you are welcome to communicate in the comment area we open the video template the tushen workflow inside this is closed. The workflow of ITV is almost the same as that of T2V. Here is the workflow of the original model. Above is the workflow with for steps of Laura. It volts is a very common video memory. Then we will use the above to speed up the workflow. Then this interface in latent he has more image loading nodes loaded into latent through this node. The rest are almost the same as T2V. Load the material to be used this time. A climbing lady. Let's animate the climb. Then enter the animation content we want. A beautiful young woman is climbing a mountain. Hair flutters naturally. I can see his face clearly. The lens has a slight handheld feel. ITV's Qwords are relatively simple because there are already specific people in the image. There is also the environmental background. Therefore, when we enter the prompt word, it's mainly descriptive, a state of motion between characters and the camera. Let's delete these in the model here. It is recommended to use a more cost effective GGUF model. Model loading node for loading GGUF and clip nodes model here and above. Select the high noise model. I have the Q8 version here. Load the low noise model below. Load the corresponding clip. Switch to when to make the corresponding connection. Respecify these models. The path includes the path of Laura here. Below this node, we can specify video resolution and video length. Change it to 720. Here maintain the 81 pin K sampler. We first fix the seed as a easy to compare. Then you can click to generate 145 seconds and 190 to seconds for the two samplers respectively. This is the result of our generated video. You can see the amplitude of the animation here. Still relatively small. That's because of the four-step lura here. It undercuts one of our animated representations. Then we can upgrade our workflow. So first we're going to solve the animation the problem of too smaller range. In the high noise version here we disable this lura. Change the total number of steps to 12 steps. In the high noise version we enable 0 to 10 steps. 12 steps. Here the number of steps is 10 to 12 steps. In this way we will not enable Lauren high noise. It guarantees that our animation amplitude is relatively large. We uniformly change the value of shift to 8. The CFG value here for high noise is changed to 3.5 left unchanged on the right. Then we add the accelerated patch. Sage attention patch and this node. Sage attention selection automatically. Both need to be enabled. Let's plug in the high noise model first. Then make another copy. Reconnect to the low noise model. We use sage attention. Accelerate our work. Then save the node. You can also switch here using video merge nodes. Change the frame rate to 16. Make corresponding changes. After optimization, we will click run again. Our improved workflow indeed more than the previous animation is much larger. Look at the generation time. The second sampler is noticeably faster. But the first sampler because there's a lot more walking. This speed can never be suppressed. is there anyway. We can both guarantee the quality of the generation but also guarantee the speed of our generation. KJ1 video rapper a plug-in that is very important to the WAN workflow. He used torch.compile single quotes s acceleration function integrate into the WAN workflow to achieve significant acceleration. It also optimizes the quality of the generated video. We can install this node through the manager workflow for this node. It also needs some supporting models. We can find it on his official website. Select to download. After downloading the plug-in, it can be found in the template. Find his official workflow here. We search for tutu to here. All workflow templates for when 2.2 can be found. This is the only one for B workflow for I2V. You don't need to look at the 5B here at all. Another IV use TTM technology. We'll talk about that later. Let's open it first. The core of this I2V workflow. This workflow is compared to the official I2V workflow. He has several advantages. First, he managed to compile torch. An acceleration effect was added to it. Load for the first time after opening requires some compilation time. When generated later, it accelerates by about 30%. Second, he is using the acceleration Laura at the same time optimized the animation amplitude of the generated video. We will explain the principle later. And thirdly, he adds a chunk node like this. When the video we generate is relatively large, easy to explode video memory. After opening him, it can reduce the burden of our video memory. Successful video generation. This is very important for generating large videos. The fourth advantage, these specialized models he provided, its quality much better than the official default model including this optimized acceleration Laura. The fifth one he put tov. The first and last framework flow is a fusion. It's more convenient for us to use. Then we come to the specific use. First, we look at the model pipeline of the sampler. Here we can load the corresponding model. KJ recommends using his KJ model for FP8. He said that his model was better than the official FP8 model is of better quality. If you choose this model, we will keep the default below unchanged. Pay attention to the attention mode here. It's Sage's attention. Sage attention. The integration with the torch compile here is very good. If you don't have this installed, then you can use this SDPA. If you have installed it, choose Sage Attention first. If you don't want to use its model, do you want to use other models? For example, the GGUF version. After selection, the third one, the type of quantification must be changed to disabled. If you want to use the official one for B model, this should also be changed to disabled. Change the words here to FP16. Low noise below. Model loading is exactly the same as above. Here we better switch back to the high noise version of the KJ model he recommended. Change back here the parameter corresponding to this name. Look, it's enabled in front of him. This function of torch compile acceleration. As long as your torch version is greater than 2.0, he can support it. Many computers enable this function by default. If you run it and report an error that proves your computer this feature was not successfully installed. The installation of torch compile. I have encountered the most complex environment installation. If necessary, I will give a separate cause later to explain his installation in detail. Now, of course, if he tells you to report an error, then just disable it. The back workflow can also be passed. The parameters here, his default parameters are very good. The first point we usually need to change is here. Full graph pattern. After opening, he will also accelerate our workflow. But this model has an extra success rate. If it is said to start the operation and report an error, then close it. Usually in video work, his success rate is relatively high. Fourth, dynamic mode. If you need to change frequently the resolution of the generated video, in other words, the frame rate, then when this is turned on, it will speed up. If you are here, if the resolution and frame rate are relatively fixed, that shutdown is acceleration. The third thing to modify is your own video memory limit. If you want to generate a large resolution video, then this needs to be increased. For example, I want to apply a width and height of 1,024, then I usually change this to 128. Others are best left unchanged by default. In this case, my resolution remains the same. Then I will only turn on the full picture mode on this side of the model pipe. It's the chunking function here. I am a forg video memory running 720 is completely fine. Then turn off this chunking function. It will greatly improve the production speed of my workflow. If I talk about my future cases is to open a 10 to 4. Then this must be turned on otherwise the workflow will not work at all. This is a loading node of Laura. We can use this interface to load the lura we want to use. Then the words here are using to accelerated loras. The top is connected to the high noise pipe. The bottom is connected to the low noise. This pipeline here is the same fourstep acceleration lura. Here KJ recommends this lura. Also from his official website when the path of this lura adjust to our local path it does not divide into high noise and low noise versions. We can load it directly. The parameters here are a little higher in intensity. The reason will be explained later. When we want to use multiple luras, then we can directly copy make another series. Then we choose again corresponding to a lura. If the parameters are used, we will change it back to one. This is another way to load lura. And then look at a second image of a pipe. We load the image we want to use. Same lady. This node is the adjustment of the image resolution and cropping. He scaled the image to 720 to OP. Then the final video is a video of the 720. That's fine. The original image is indeed a bit too large. This node at the back, it is mainly used to adjust the frame number. Don't touch it for other parameters. There is a endore image interface here. If you want to do end to end frame workflow, you can plug it in. We'll explain later. Load the VA interface here. Then we will use this VA. And the third is a Qword interface. Then enter a prompt word we want here. Then we enter the same prompt word as before. Here is the loading of the clip. This is a clip recommended by KJ. Others remain unchanged. Go back to the K sampler. Let's leave the following interfaces alone. And then the step count is down there. Its steps are no longer for steps but increased to six steps because of this acceleration. Laura, the recommended number of steps is for six. When there are six steps, the quality is very high. One step higher. No need to increase. Then there is the CFG value. The CFG value of the first sampler. Use this node. It essentially executes a variable one. CFG value here parameters. Let's not touch it. To understand what it means, we can run it first. Then we'll close it when we get here. Our step count. There are six steps in total. The CFG value of the first step is two. The 2 3 4 5 6 CFG value in the back is one. The reason why it adopts such a dynamic CFG value mainly because this four-step acceleration law, it requires a CFG value of one. But this number one will cause our range of motion to be too small. Therefore he initially the first step to adopt is two step Laura. The first step is to compose the picture and movement is the greatest impact. The back mainly affects the quality of the picture. Combine this with a strength of three of Laura. Through these parameters it will allow the animation of the generated video. It gets a lot bigger. And the second sampler cfg value is always one. It's mainly for quality control not animation. The value of shift is eight. As for seeds I am more used to fixing them as one. The following scheduleuler is DPM++ seed. In fact, it is essentially a sampling collection of devices and scheduulers. He has put some commonly used samplers and scheduulers do an integration. We generally keep his default unchanged. Here are some parameters. We also keep the default unchanged. So, the good news here is that this is a chunk decoding. When our video resolution is particularly large, we can turn on this chunked decoding. Then I'll keep it closed. Finally, we save it as a video. So this is a complete analysis of the workflow. Then after we adjust all the parameters and we go straight into action. On the left is the video generated by us. On the right is the video generated by the official workflow. It is obvious that KJ's workflow has a greater range of motion and the image quality is clearer. In terms of time, even though we're doing sixst step generation, but really fast 100 to seconds and 91 seconds, this workflow of KJ, it does improve the generation quality of the picture on the one hand. On the other hand, it increases speed. This is the official end to end frame workflow. We click on it compared to the I2V workflow. One more image input and the overall animation logic is from the image one. Switch to image two end to end frame. The scope of application of workflow is quite large. For example, these two. We can load the image of the product to make a conversion between products. In the field of e-commerce and product packaging, there is a good application. It is more commonly used because when we are doing I2V, the frame number we generate by default is only 81 frames. When we increase this frame number, many times it will reduce the quality of the generation. Even directly reporting the video memory cannot generate pictures. A good way to continue our video is to use the end to end frame workflow. Let's do it in detail. Go back to this workflow of KJ. We operate from here. Copy. Then put the picture below. Connect it to end image here. This is the end to end frame workflow. In the second picture, I will change it to take a selfie and wave this picture on the mountain. This figure I generated with context. Let's take a look at this workflow. Flux's context model. He does a great job of achieving consistency in his characters. I type this girl and I type in what I want to change. Let her stand on the hill and wave. This prompt word only supports English if you want to learn a bit about him. Usage my class will definitely help you. Well, what is different from the official workflow is I added torch compile app to the model to accelerate. Here is the sage attention patch. They're very well connected sampler anduler. I used a high quality combination. If you are not safe, if you install the plug-in for this sampler, you use the official default sampler is also possible. This graph is the result of our generation. There's a problem. Here's the text. Context support for Chinese characters is very poor. At this point, I need to fix it. I use image models come to our images repair because his understanding of Chinese characters is very good. This workflow is essentially workflow for partial repair. But because the resolution of this text is too small, cause me to fix it and keep failing. So I use a node like this. Come and help me fix it. For his use, I have a detailed explanation in this lesson. If you don't want to go to so much trouble, so you want to fix small print like this and you can absolutely zoom in first, fix it again. It also works after the repair is done. Add it to us. We don't need to touch it for other parameters. You can click to run directly. Then we say get the result we want to generate. Then I used context to generate the next image. Then I hope this girl after waving this mask enters our screen. We're going to do a video serial like this. Then I need to extract the last frame of this video as a starting frame here. So let's load this image first. We load the video we just generated. There are two ways for us to get the last frame. The first method the number of frames loaded here we change to one then skip the first 79 frames so that we get the picture of the last frame don't jump to 80 and it's gone. We can drag a preview node to take a look and when we look closer we see that image color something else has happened. We use changing images as this starting frame continuity of two videos there's going to be problems. We don't usually use this method to load this last frame. The best way is to generate it directly. Inside we directly generate the image of the last frame. Find it in the plugin of video assistant image finds this node. The following is the split index. We have a total of 81 frames. If the index is 10. So our first interface, it will output the first 10 frames. The second interface will output other frames. We just want the last frame. There is a special method. Just change this to minus one. In this way, our interface B output is directly the last frame. So here we have a save image interface and then plug that in a little bit more. We directly get the complete last frame of image. No color difference will occur. We'll run it again. After we get the last frame, load the last frame on this image. Load this picture of the group photo. This way we can continue to generate. We currently have three such videos. To get the final good result, we need to zoom in on three videos. Then merge them together and do it again. Make up the frames and finally get a highquality smooth video. Video amplification. Highly recommend seed V2 as a plug-in. We use video to amplify the workflow. This workflow is also enabled. This node of torch compile. If you successfully install it, this node can be opened for accelerated loading. Video to upload. Turn on full picture mode as much as possible because the video duration is relatively short so we don't increase it. The model here prefers high quality 7B. We choose this belt of 7B. Sharpen this model. The cache model turns on sage attention. Sage attention to the following words. Let's turn on this caching model. Enter the resolution of the smallest side here. Then I want this smallest side to be 1080. Click run to get an enlarged video. After zooming in, let's combine these three videos into one. Use this connected image node of the video assistant. The first video is connected to channel A. The second video is connected to channel B. The final image we save as. Use the video merge node to connect. 16 frames back and forth. Do the basic settings. The format here is generally MP4 is enough, but you can also pay a little attention to H264. It's lossy compression. You want to get lossless compressed video, you can choose the bottom two. I personally recommend the last one. It is a free and open- source also. We need to merge three videos. So, we need to make another copy. This tunnel leads to a tunnel here. This is connected to be again. Then this interface will be connected by him. This is how to put these three videos merged together. Then click run. This the video merging in is quite fast. After the merger, we going to be talking about frame filling. Increase the smoothness of the image. We need to install this makeup frame node. It is one of the best plugins for video makeup frames at present. It provides a lot of some makeup schemes. Applicable to various video types. We can install it through the manager. In addition, the type of each makeup frame. You need to download the corresponding model to enable it. After the installation is completed, we open this node. Open the VFI folder. There are many plugins for making up frames. The most common is this plug-in for this RI VFI. It is very stable and reliable as a complimentary frame node. It is also recommended to use the Inet VFI node. It is for this node make an upgraded one node effect. Its quality will be better but the generation speed will be slower. The third recommendation is the GN FSS node. It is a complimentary frame node specifically for animation. animated videos would be better to use it. What about here? So, I'm going to use the traditional node here for demonstration. Connect the generated image directly here. Then merge this video. Connect the node here. The first thing to choose is the model of the supplementary frame here. The most recommended model is the RI for 7 model. Its stability comprehensive compatibility is the best. If you are a 50 series graphics card, if the video memory is better, that's the recommended model. Other models are not recommended. Then the second the default is to clear the cache every 10 frames. If your video memory is relatively low then this value can be adjusted to around five but it will increase working hours. The third very core it is a multiple of the supplementary frame. If it is 2 80 frames as an example our result becomes 160 frames. If it is three it is 80 * 3 which is 240 frames. Usually two is enough. Don't touch it for other parameters. Let's click run again. Successfully make up frames for the picture. If you still want to do something with the video for color matching, it is highly recommended to use caput scissors software for color grading. If you say the format of your merged video nodes, if it has always been in this last format, then when you debug in the latest software, we'll be more comfortable. It's not easy to do individual teaching. If this video has helped you, please like and support us a lot. Thanks a lot.
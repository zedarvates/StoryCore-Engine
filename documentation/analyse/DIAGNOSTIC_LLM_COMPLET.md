# Diagnostic Complet - Probl√®me LLM Non Connect√©

## üìã Table des Mati√®res

1. [R√©sum√© Ex√©cutif](#r√©sum√©-ex√©cutif)
2. [Analyse Technique](#analyse-technique)
3. [Solutions Rapides](#solutions-rapides)
4. [Outils de Diagnostic](#outils-de-diagnostic)
5. [Guide de R√©solution](#guide-de-r√©solution)
6. [Pr√©vention Future](#pr√©vention-future)

---

## R√©sum√© Ex√©cutif

### Probl√®me Identifi√©

Les fonctionnalit√©s LLM (chatbox, assistants IA, g√©n√©ration automatique) ne fonctionnent pas car le service LLM n'est pas correctement initialis√© ou configur√©.

### Impact

- ‚ùå Chatbox non fonctionnelle
- ‚ùå Assistants IA d√©sactiv√©s
- ‚ùå G√©n√©ration automatique impossible
- ‚ùå Wizards sans suggestions IA

### Causes Principales (par ordre de probabilit√©)

1. **Configuration LLM manquante** (90%)
2. **API Key absente ou invalide** (70%)
3. **Erreur de chiffrement** (40%)
4. **Ollama non d√©marr√©** (20% si provider = local)
5. **Service non propag√© aux composants** (30%)

### Solution Rapide (5 minutes)

**Option A: Ollama (Gratuit)**
```bash
# 1. Installer Ollama
# https://ollama.ai

# 2. D√©marrer
ollama serve

# 3. T√©l√©charger un mod√®le
ollama pull gemma3:1b

# 4. Configurer dans l'app
# Settings ‚Üí Provider: Local, Model: gemma3:1b
```

**Option B: OpenAI (Payant)**
```
1. Obtenir API key: https://platform.openai.com/api-keys
2. Settings ‚Üí Provider: OpenAI, Model: gpt-3.5-turbo
3. Entrer l'API key
```

---

## Analyse Technique

### Architecture du Service LLM

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Application React                        ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ         LandingChatBox Component                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  LLMService Instance                       ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Provider: openai/anthropic/local       ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Config: model, apiKey, parameters      ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Methods: generateCompletion()          ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  State:                                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - llmService: LLMService | null                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - llmConfig: LLMConfig                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - connectionStatus: 'online' | 'offline' | ...      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - isFallbackMode: boolean                           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ         LocalStorage                                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - storycore_llm_config (config sans API key)       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - storycore_api_key_enc (API key chiffr√©e)         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ         SessionStorage                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - storycore_encryption_key (cl√© de chiffrement)    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LLM Providers                            ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ   OpenAI     ‚îÇ  ‚îÇ  Anthropic   ‚îÇ  ‚îÇ    Ollama    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ api.openai   ‚îÇ  ‚îÇ api.anthropic‚îÇ  ‚îÇ localhost:   ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    .com      ‚îÇ  ‚îÇ    .com      ‚îÇ  ‚îÇ    11434     ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flux d'Initialisation

```
1. Page Load
   ‚îÇ
   ‚îú‚îÄ> useEffect: initializeConfiguration()
   ‚îÇ   ‚îÇ
   ‚îÇ   ‚îú‚îÄ> autoMigrate() (migration Ollama si n√©cessaire)
   ‚îÇ   ‚îÇ
   ‚îÇ   ‚îú‚îÄ> loadConfiguration() (depuis localStorage)
   ‚îÇ   ‚îÇ   ‚îÇ
   ‚îÇ   ‚îÇ   ‚îú‚îÄ> D√©chiffrer API key
   ‚îÇ   ‚îÇ   ‚îî‚îÄ> Retourner LLMConfig
   ‚îÇ   ‚îÇ
   ‚îÇ   ‚îú‚îÄ> checkOllamaStatus()
   ‚îÇ   ‚îÇ
   ‚îÇ   ‚îî‚îÄ> setLlmConfig(config)
   ‚îÇ
   ‚îî‚îÄ> useEffect: Initialize LLM Service
       ‚îÇ
       ‚îú‚îÄ> V√©rifier si API key requise
       ‚îÇ   ‚îÇ
       ‚îÇ   ‚îú‚îÄ> Si manquante ‚Üí Mode fallback
       ‚îÇ   ‚îî‚îÄ> Si pr√©sente ‚Üí Continuer
       ‚îÇ
       ‚îú‚îÄ> new LLMService(llmConfig)
       ‚îÇ
       ‚îî‚îÄ> setLlmService(service)
```

### Points de D√©faillance

#### 1. Configuration Non Charg√©e

**Fichier:** `creative-studio-ui/src/utils/llmConfigStorage.ts`

```typescript
export async function loadConfiguration(): Promise<ChatboxLLMConfig | null> {
  try {
    const configJson = localStorage.getItem(STORAGE_KEYS.LLM_CONFIG);
    const encryptedApiKey = localStorage.getItem(STORAGE_KEYS.API_KEY_ENCRYPTED);

    if (!configJson) {
      return null; // ‚ùå Aucune configuration
    }

    const storedConfig: StoredLLMConfig = JSON.parse(configJson);
    const apiKey = encryptedApiKey ? await decryptAPIKey(encryptedApiKey) : '';

    return {
      provider: storedConfig.provider,
      model: storedConfig.model,
      temperature: storedConfig.temperature,
      maxTokens: storedConfig.maxTokens,
      apiKey, // ‚ùå Peut √™tre vide
      streamingEnabled: storedConfig.streamingEnabled,
    };
  } catch (error) {
    console.error('Failed to load LLM configuration:', error);
    return null; // ‚ùå Erreur de chargement
  }
}
```

#### 2. Service Non Instanci√©

**Fichier:** `creative-studio-ui/src/components/launcher/LandingChatBox.tsx`

```typescript
useEffect(() => {
  const requiresApiKey = llmConfig.provider === 'openai' || llmConfig.provider === 'anthropic';
  
  if (requiresApiKey && !llmConfig.apiKey) {
    setLlmService(null); // ‚ùå Service non cr√©√©
    setConnectionStatus('fallback');
    setIsFallbackMode(true);
    return;
  }

  const service = new LLMService(llmConfig);
  setLlmService(service); // ‚úì Service cr√©√©
  
  setConnectionStatus('online');
  setIsFallbackMode(false);
}, [llmConfig]);
```

#### 3. Erreur de Chiffrement

**Fichier:** `creative-studio-ui/src/utils/llmConfigStorage.ts`

```typescript
export async function decryptAPIKey(encryptedData: string): Promise<string> {
  if (!encryptedData) {
    return '';
  }

  try {
    const key = await getEncryptionKey(); // ‚ùå Peut √©chouer si cl√© perdue
    
    const [encryptedBase64, ivBase64] = encryptedData.split(':');
    if (!encryptedBase64 || !ivBase64) {
      throw new Error('Invalid encrypted data format'); // ‚ùå Format invalide
    }

    // D√©chiffrement...
    return decoder.decode(decrypted);
  } catch (error) {
    console.error('API key decryption failed:', error);
    throw new Error('Failed to decrypt API key'); // ‚ùå √âchec du d√©chiffrement
  }
}
```

### D√©pendances Critiques

```
LLMService
  ‚îú‚îÄ> LLMConfig (configuration)
  ‚îÇ   ‚îú‚îÄ> provider (openai/anthropic/local/custom)
  ‚îÇ   ‚îú‚îÄ> model (nom du mod√®le)
  ‚îÇ   ‚îú‚îÄ> apiKey (cl√© API chiffr√©e)
  ‚îÇ   ‚îî‚îÄ> parameters (temp√©rature, tokens, etc.)
  ‚îÇ
  ‚îú‚îÄ> LLMProviderBase (classe abstraite)
  ‚îÇ   ‚îú‚îÄ> OpenAIProvider
  ‚îÇ   ‚îú‚îÄ> AnthropicProvider
  ‚îÇ   ‚îî‚îÄ> CustomProvider (Ollama)
  ‚îÇ
  ‚îî‚îÄ> Web Crypto API (pour chiffrement)
      ‚îî‚îÄ> sessionStorage (cl√© de chiffrement)
```

---

## Solutions Rapides

### Solution 1: Configuration Initiale (Ollama)

**Temps:** 5-10 minutes  
**Co√ªt:** Gratuit  
**Difficult√©:** Facile

```bash
# √âtape 1: Installer Ollama
# Windows: T√©l√©charger depuis https://ollama.ai
# macOS: brew install ollama
# Linux: curl -fsSL https://ollama.ai/install.sh | sh

# √âtape 2: D√©marrer Ollama
ollama serve

# √âtape 3: T√©l√©charger un mod√®le (dans un nouveau terminal)
ollama pull gemma3:1b

# √âtape 4: V√©rifier l'installation
curl http://localhost:11434/api/tags

# √âtape 5: Configurer dans l'application
# Ouvrir Settings ‚Üí LLM Configuration
# - Provider: Local
# - Model: gemma3:1b
# - Endpoint: http://localhost:11434
# - Streaming: Activ√©
# Cliquer sur "Save"

# √âtape 6: Tester
# Ouvrir la chatbox et envoyer un message
```

### Solution 2: Configuration Initiale (OpenAI)

**Temps:** 2-3 minutes  
**Co√ªt:** Payant (√† partir de $0.001/1K tokens)  
**Difficult√©:** Tr√®s facile

```
√âtape 1: Obtenir une API key
  ‚Üí https://platform.openai.com/api-keys
  ‚Üí Cliquer sur "Create new secret key"
  ‚Üí Copier la cl√© (sk-...)

√âtape 2: Configurer dans l'application
  ‚Üí Ouvrir Settings ‚Üí LLM Configuration
  ‚Üí Provider: OpenAI
  ‚Üí Model: gpt-3.5-turbo (√©conomique) ou gpt-4
  ‚Üí API Key: Coller la cl√©
  ‚Üí Streaming: Activ√©
  ‚Üí Cliquer sur "Save"

√âtape 3: Tester
  ‚Üí Ouvrir la chatbox
  ‚Üí Envoyer un message
  ‚Üí V√©rifier la r√©ponse
```

### Solution 3: R√©initialisation Compl√®te

**Temps:** 1 minute  
**Utilisation:** Configuration corrompue

```javascript
// Ouvrir la console du navigateur (F12)

// Supprimer toute la configuration
localStorage.removeItem('storycore_llm_config');
localStorage.removeItem('storycore_api_key_enc');
sessionStorage.removeItem('storycore_encryption_key');

// Recharger la page
location.reload();

// Reconfigurer (voir Solution 1 ou 2)
```

### Solution 4: Diagnostic Automatique

**Temps:** 30 secondes  
**Utilisation:** Identifier le probl√®me

```javascript
// Ouvrir la console du navigateur (F12)

// Importer et ex√©cuter le diagnostic
import { runLLMDiagnostic, printDiagnostic } from './src/utils/llmDiagnostic';
const result = await runLLMDiagnostic();
printDiagnostic(result);

// Suivre les recommandations affich√©es
```

---

## Outils de Diagnostic

### 1. Panneau de Diagnostic Visuel

**Fichier:** `creative-studio-ui/src/components/debug/LLMDiagnosticPanel.tsx`

**Utilisation:**

```typescript
import { LLMDiagnosticPanel } from '@/components/debug/LLMDiagnosticPanel';

function MyComponent() {
  const [showDiagnostic, setShowDiagnostic] = useState(false);

  return (
    <>
      <Button onClick={() => setShowDiagnostic(true)}>
        üîç Diagnostic LLM
      </Button>

      {showDiagnostic && (
        <LLMDiagnosticPanel 
          onClose={() => setShowDiagnostic(false)}
          onOpenSettings={() => {
            setShowDiagnostic(false);
            // Ouvrir le dialogue de configuration
          }}
        />
      )}
    </>
  );
}
```

**Fonctionnalit√©s:**
- ‚úì Tests automatiques (storage, crypto, connectivity, etc.)
- ‚úì Affichage visuel des r√©sultats
- ‚úì Recommandations personnalis√©es
- ‚úì Export des r√©sultats en JSON
- ‚úì Copie dans le presse-papiers

### 2. Badge de Statut

**Fichier:** `creative-studio-ui/src/components/debug/LLMDiagnosticPanel.tsx`

**Utilisation:**

```typescript
import { LLMDiagnosticBadge } from '@/components/debug/LLMDiagnosticPanel';

function NavigationBar() {
  return (
    <nav>
      {/* ... autres √©l√©ments ... */}
      <LLMDiagnosticBadge onClick={() => setShowDiagnostic(true)} />
    </nav>
  );
}
```

**Affichage:**
- üü¢ LLM Healthy (tout fonctionne)
- üü° LLM Warning (probl√®mes mineurs)
- üî¥ LLM Error (probl√®mes critiques)
- üîµ Checking... (diagnostic en cours)

### 3. Utilitaire de Diagnostic

**Fichier:** `creative-studio-ui/src/utils/llmDiagnostic.ts`

**Fonctions disponibles:**

```typescript
// Diagnostic complet
const result = await runLLMDiagnostic();
// Retourne: DiagnosticResult avec tous les tests

// Afficher dans la console
printDiagnostic(result);

// V√©rification rapide
const isHealthy = await isLLMHealthy();
// Retourne: boolean

// Message de statut
const message = await getLLMStatusMessage();
// Retourne: string (message format√©)
```

### 4. Page de Test HTML

**Fichier:** `test-llm-connection.html`

**Utilisation:**
```bash
# Ouvrir dans un navigateur
open test-llm-connection.html

# Ou avec un serveur local
python -m http.server 8080
# Puis ouvrir: http://localhost:8080/test-llm-connection.html
```

**Tests effectu√©s:**
1. LocalStorage (disponibilit√© et contenu)
2. Web Crypto API (chiffrement)
3. Ollama (connexion locale)
4. OpenAI API (si configur√©e)
5. Validit√© de la configuration

---

## Guide de R√©solution

### Sc√©nario 1: Premi√®re Installation

**Sympt√¥mes:**
- Aucune configuration LLM
- Boutons IA d√©sactiv√©s
- Message "Configure LLM settings"

**Diagnostic:**
```javascript
localStorage.getItem('storycore_llm_config') === null
```

**Solution:**
1. Choisir un provider (Ollama recommand√© pour d√©buter)
2. Suivre Solution 1 ou Solution 2 ci-dessus
3. Tester la chatbox

### Sc√©nario 2: Configuration Corrompue

**Sympt√¥mes:**
- Erreurs de d√©chiffrement
- Configuration existe mais ne fonctionne pas
- Erreurs "Failed to decrypt"

**Diagnostic:**
```javascript
// Configuration existe
localStorage.getItem('storycore_llm_config') !== null

// Mais erreur au chargement
await loadConfiguration() === null
```

**Solution:**
1. Suivre Solution 3 (R√©initialisation)
2. Reconfigurer le LLM

### Sc√©nario 3: Ollama Non Connect√©

**Sympt√¥mes:**
- Provider = Local
- Erreur "Failed to connect"
- Timeout errors

**Diagnostic:**
```bash
curl http://localhost:11434/api/tags
# Erreur: Connection refused
```

**Solution:**
```bash
# D√©marrer Ollama
ollama serve

# V√©rifier
curl http://localhost:11434/api/tags

# Si aucun mod√®le
ollama pull gemma3:1b
```

### Sc√©nario 4: API Key Invalide

**Sympt√¥mes:**
- Provider = OpenAI/Anthropic
- Erreur "Authentication failed"
- Status 401

**Diagnostic:**
```bash
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer YOUR_API_KEY"
# Erreur: 401 Unauthorized
```

**Solution:**
1. V√©rifier l'API key sur le dashboard du provider
2. V√©rifier les cr√©dits disponibles
3. G√©n√©rer une nouvelle cl√© si n√©cessaire
4. Mettre √† jour dans Settings

### Sc√©nario 5: Service Non Propag√©

**Sympt√¥mes:**
- LandingChatBox fonctionne
- Wizards/Assistants ne fonctionnent pas
- Service LLM non accessible dans d'autres composants

**Diagnostic:**
```typescript
// Dans LandingChatBox
llmService !== null // ‚úì

// Dans GenericWizardModal
llmService === undefined // ‚úó
```

**Solution:**
Cr√©er un contexte React global (voir section Pr√©vention Future)

---

## Pr√©vention Future

### 1. Cr√©er un Contexte LLM Global

**Fichier:** `creative-studio-ui/src/contexts/LLMContext.tsx`

```typescript
import React, { createContext, useContext, useState, useEffect } from 'react';
import { LLMService, type LLMConfig } from '@/services/llmService';
import { loadConfiguration, saveConfiguration } from '@/utils/llmConfigStorage';

interface LLMContextValue {
  llmService: LLMService | null;
  llmConfig: LLMConfig | null;
  isConfigured: boolean;
  isConnected: boolean;
  updateConfig: (config: LLMConfig) => Promise<void>;
  testConnection: () => Promise<boolean>;
}

const LLMContext = createContext<LLMContextValue | null>(null);

export function LLMProvider({ children }: { children: React.ReactNode }) {
  const [llmService, setLlmService] = useState<LLMService | null>(null);
  const [llmConfig, setLlmConfig] = useState<LLMConfig | null>(null);
  const [isConnected, setIsConnected] = useState(false);

  // Initialisation au montage
  useEffect(() => {
    async function init() {
      const config = await loadConfiguration();
      if (config) {
        await updateConfig(config);
      }
    }
    init();
  }, []);

  const updateConfig = async (config: LLMConfig) => {
    setLlmConfig(config);
    
    // Cr√©er le service
    const service = new LLMService(config);
    setLlmService(service);
    
    // Tester la connexion
    const connected = await testConnection();
    setIsConnected(connected);
    
    // Sauvegarder
    await saveConfiguration(config);
  };

  const testConnection = async (): Promise<boolean> => {
    if (!llmService) return false;
    
    const result = await llmService.validateConnection();
    const connected = result.success && result.data === true;
    setIsConnected(connected);
    return connected;
  };

  return (
    <LLMContext.Provider value={{
      llmService,
      llmConfig,
      isConfigured: !!llmService,
      isConnected,
      updateConfig,
      testConnection
    }}>
      {children}
    </LLMContext.Provider>
  );
}

export function useLLM() {
  const context = useContext(LLMContext);
  if (!context) {
    throw new Error('useLLM must be used within LLMProvider');
  }
  return context;
}
```

**Utilisation:**

```typescript
// Dans App.tsx
import { LLMProvider } from '@/contexts/LLMContext';

function App() {
  return (
    <LLMProvider>
      {/* Votre application */}
    </LLMProvider>
  );
}

// Dans n'importe quel composant
import { useLLM } from '@/contexts/LLMContext';

function MyComponent() {
  const { llmService, isConfigured, isConnected } = useLLM();

  if (!isConfigured) {
    return <div>Please configure LLM</div>;
  }

  if (!isConnected) {
    return <div>LLM is not connected</div>;
  }

  // Utiliser llmService...
}
```

### 2. Ajouter une Validation Automatique

**Fichier:** `creative-studio-ui/src/hooks/useLLMValidation.ts`

```typescript
import { useEffect, useState } from 'react';
import { useLLM } from '@/contexts/LLMContext';

export function useLLMValidation(interval: number = 60000) {
  const { llmService, testConnection } = useLLM();
  const [lastCheck, setLastCheck] = useState<Date | null>(null);

  useEffect(() => {
    if (!llmService) return;

    // Test initial
    testConnection().then(() => setLastCheck(new Date()));

    // Test p√©riodique
    const timer = setInterval(async () => {
      await testConnection();
      setLastCheck(new Date());
    }, interval);

    return () => clearInterval(timer);
  }, [llmService, interval]);

  return { lastCheck };
}
```

### 3. Am√©liorer les Messages d'Erreur

**Fichier:** `creative-studio-ui/src/components/LLMErrorBoundary.tsx`

```typescript
import React, { Component, ReactNode } from 'react';
import { LLMError } from '@/services/llmService';

interface Props {
  children: ReactNode;
  fallback?: (error: Error) => ReactNode;
}

interface State {
  hasError: boolean;
  error: Error | null;
}

export class LLMErrorBoundary extends Component<Props, State> {
  constructor(props: Props) {
    super(props);
    this.state = { hasError: false, error: null };
  }

  static getDerivedStateFromError(error: Error): State {
    return { hasError: true, error };
  }

  componentDidCatch(error: Error, errorInfo: React.ErrorInfo) {
    console.error('LLM Error:', error, errorInfo);
  }

  render() {
    if (this.state.hasError && this.state.error) {
      if (this.props.fallback) {
        return this.props.fallback(this.state.error);
      }

      const llmError = this.state.error instanceof LLMError 
        ? this.state.error 
        : new LLMError(this.state.error.message, 'unknown');

      return (
        <div className="p-4 bg-red-50 border border-red-200 rounded">
          <h3 className="font-bold text-red-800">LLM Error</h3>
          <p className="text-red-600">{llmError.getUserMessage()}</p>
          <ul className="mt-2 text-sm text-red-700">
            {llmError.getSuggestedActions().map((action, i) => (
              <li key={i}>‚Ä¢ {action}</li>
            ))}
          </ul>
        </div>
      );
    }

    return this.props.children;
  }
}
```

---

## Fichiers de Documentation

### Fichiers Cr√©√©s

1. **ANALYSE_PROBLEME_LLM.md**
   - Analyse technique compl√®te
   - Causes d√©taill√©es
   - Solutions par bug

2. **GUIDE_RESOLUTION_RAPIDE_LLM.md**
   - Guide pas-√†-pas
   - Solutions par sc√©nario
   - Commandes de debug

3. **RESUME_PROBLEME_LLM.txt**
   - R√©sum√© visuel ASCII
   - Checklist rapide
   - Actions imm√©diates

4. **DIAGNOSTIC_LLM_COMPLET.md** (ce fichier)
   - Documentation compl√®te
   - Architecture technique
   - Pr√©vention future

5. **creative-studio-ui/src/utils/llmDiagnostic.ts**
   - Utilitaire de diagnostic
   - Tests automatiques
   - Fonctions d'aide

6. **creative-studio-ui/src/components/debug/LLMDiagnosticPanel.tsx**
   - Composant de diagnostic visuel
   - Badge de statut
   - Interface utilisateur

7. **test-llm-connection.html**
   - Page de test standalone
   - Tests interactifs
   - Export de r√©sultats

---

**Date:** 2026-01-20  
**Version:** 1.0  
**Auteur:** Kiro AI Assistant  
**Statut:** Complet et pr√™t √† l'utilisation

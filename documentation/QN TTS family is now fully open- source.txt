
The QN TTS family is now fully open- source. And I'm not talking about some basic text to speech here. This thing is legitimately competing with premium closed source models like 11 Labs and Miniax. And in some cases, it's actually beating them. We're talking voice design from text prompts, voice cloning from just a few seconds of audio, multilingual support across 10 languages, and the craziest part, you can run this locally.
Before we dive into the demos, let's talk numbers because these benchmarks are wild. When they tested this against 11 Labs, Miniax, and other premium models, Qentts achieved 15% lower word error rates in multilingual tests compared to 11 Labs and GPT40 audio. On Chinese English stability tests, it's hitting state-of-the-art performance, actually surpassing seed TTS, Miniax, and GPT40 audio preview.
And for speaker similarity in languages like English, Italian, and French, it's significantly outperforming both Miniax and 11 Labs. Keep in mind, those are closed source paid models we're comparing against here. The fact that an open-source model is competing at this level is honestly insane.
So, they've released two main models. The first one is the 1.7 billion parameter beast, and it comes in three flavors. There's the voice design version, where you can literally describe the voice you want through text prompts. Then there's the custom voice version with built-in tombers that you can control through instructions. And finally, the base model that handles voice cloning.
The second model is a smaller 0.6 billion parameter version. And surprisingly, even this smaller one supports voice cloning and all 10 languages. The only thing it's missing is instruction control, but for the size, that's pretty impressive. Let me show you what this voice design feature actually does. So, here I'm looking at an example where they gave it a superdetailed prompt.
British accent, male voice, specific pitch, speed, clarity, and even personality traits. Check out what it generated. Nine different exciting ways of cooking sausage. Incredible. There were three outstanding deliveries in terms of the sausage being the hero. The first dish that we want to dissect, this individual smartly combined different proteins in their sausage. Great seasoning.
The blend was absolutely spot-on. Congratulations. Please step forward. >> As you're hearing, this doesn't sound like typical robotic TTS at all. There's actual emotion in there. It's fluid. It's natural. Now, look at this Chinese example. It's a sad crying voice. I don't speak Chinese, but you can absolutely hear the emotion coming through.
The voice design is that precise. Here's where it gets really interesting. Look at this example where they gave it an instruction to start with a laugh. The actual text they wanted it to speak didn't include any laughing notation, but because the voice design prompt said after an initial laugh, it actually laughed at the beginning before speaking the text.
Good one. Okay, fine. I'm just going to leave this sock monkey here. Goodbye. That means you can layer in emotions and behaviors just through your instruction prompt even if they're not in the script itself. That's a gamecher for content creation. And check out this one. They wanted maximum human likeness.
So they added filler words into the script like all those natural pauses we do when we speak. Listen to how natural this sounds. Yeah. So uh I'm a digital nomad, right? So pretty much all my communication is just like texts and messages. And now, you know, there's these AI agents that can uh reply for you, which is convenient, sure, I guess, but also kind of delicate, you know, like you'll type something super short like, "Yep, sounds good.
" And it'll turn that into this whole warm, polished paragraph, like way nicer than I'd ever write myself. Huh? Seriously, I sound like a Hallmark card all of a sudden. It's genuinely hard to tell this is AI generated. They also gave it complex text with numbers and capitalized letters like vote in all caps which usually trips up TTS models.
>> Lying you watching 1866 IDLE03 for JPL. That's 18664365703 or text the word vote to 5703. Diana Darmmo's next with more from the movies right after this brief intermission on American Idol. This thing handled it perfectly while maintaining the voice of a broadcast booth announcer they specified in the background prompt.
Now, let me show you the Tombber reuse feature. They've got these built-in voices about 10 different speaker models with names like Serena, Ryan, Viven, Uncle Fu. You can use these right out of the box. Look at this dialogue between two characters, Lucas and Mia. You dropped your uh calculus notebook. I mean, I think it's yours, maybe.
>> Oh, wow. my mortal enemy, Mr. Thompson's problem sets. Thanks for rescuing me from that F. >> No problem. I actually kind of finished those already. If you want to compare answers or something, >> is this your sneaky way of saying you want to study together, Lucas, because I saw you staring during lab partners sign up? >> What? No.
I mean, yes, but not like I just think your your titration technique is really precise. That's the nerdiest compliment I've ever gotten. Tell you what, help me survive pre-calc and I'll teach you how to actually flirt. >> This is almost a full minute of back and forth conversation with emotions, movie character style delivery.
Most TTS models struggle with this kind of long- form emotional dialogue, but QN is handling it smoothly. The custom voice feature lets you take one of these built-in voices and control it with instructions. So, here's Ryan's voice, and I'm going to play six different versions where the instruction changes, but the sentence stays the same.
Listen to how drastically different these sound. She said she would be here by noon. She said she would be here by noon. She said she would be here by noon. She said she would be here by noon. She said she would be here by noon. She said she would be here by noon. Same sentence, completely different emotional delivery every time.
And this works cross-linguually, too. Look, instruction in Chinese, output in Korean. Instruction in English, output in Japanese. The voice characteristics transfer across languages seamlessly. All right. Now, voice cloning. This is where it gets really fun. So for this Chinese example, they gave it a 7-second reference sample. Listen to this input.
Now check the cloned output reading a completely different text. The similarity is excellent. Let me play some English examples. >> Hi everybody. Annayia, thank you for that beautiful introduction. I could not be prouder of everything you've done in your time with the Obama Foundation. >> This one, I'm pretty sure they sampled Barack Obama's voice.
Now, listen to the cloned output. >> In the absence of confiscation, the Portuguese Inquisitors were not earnest in tracing the heresies of ancestors or in following up the records of fugitives. >> Yeah, that's unmistakably Obamaike. And here's Steve Jobs voice being cloned. Listen to the input. I dropped out of Reed College after the first six months, but then stayed around as a dropin for another 18 months or so before I really quit.
>> And now the output reading a different script. >> An ideal harmonious society. Humanism is a lighthouse on this way to guide us in case we are getting lost. >> The clone quality is seriously impressive. They even tested cross-lingual cloning. So this example takes Trump's voice in English as input, but outputs in Japanese.
Let me play that. In short, we embarked on a mission to make America great again for all Americans. Okay, there's some similarity, though it's not a perfect clone, probably because the language shift affects the voice characteristics a bit. But look at this Korean example with Chinese input and Korean output. The similarity holds up pretty well across languages.
Text robustness is another strong point. You can give it a script with mixed English and Chinese, and it'll switch between languages mid-sentence while maintaining the clone voice. Listen to this. Where it sees English, it reads English. Where it sees Chinese, it reads Chinese. Seamless. And they even tested it with mathematical equations embedded in text which is notoriously difficult for TTS.
Listen to how it handles reading this full equation. >> I am solving the equation. X= - B plus or minus the of B^ 2 - 4 A / 2 A. Nobody can. It's a disaster. Very sad. >> It pronounces the entire thing correctly. Now here's a feature I haven't seen elsewhere. Voice reconstruction.
You can take someone's voice or even a song and reconstruct it. This isn't cloning for new text. It's recreating the original audio. Listen to this dialect sample. And now the reconstructed version. They're almost identical, but the second one is AI generated. And check out this song reconstruction. Can you tell which is original and which
is AI? The similarity is uncanny. They even do background sound reconstruction where it recreates not just the voice but the ambient noise and background elements. Listen to this input with background sound. What's the matter with you two? >> And now the output. >> What's the matter with you two? >> It captured both the voice and the environment.
So, how does this actually work under the hood? Without getting too technical, Qentts uses a multi-stage architecture. It's trained on massive data sets that allow it to learn not just pronunciation, but procity, emotion, and even contextual tone adaptation. The voice design model essentially maps text descriptions to acoustic features, while the cloning model uses a speaker embedding system that captures voice characteristics from short audio samples.
The streaming capability comes from a low latency architecture with first packet latency as low as 97 milliseconds, which is why it can start speaking almost instantly. They just released all the models on both hugging face and model scope a few hours ago. Everything is under the Apache 2.0 license, so you can use it commercially. The 1.
7 billion voice design model is under 5 gigs. So, if you've got around 6 GB of VRAM, you should be able to run it locally. If you've got a beefier GPU, you can get real-time generation with that 0.97 latency ratio they mentioned. There's also a HuggingFace demo space where you can test all three versions, voice design, custom voice, and voice cloning before you install anything.
I'll drop the link to their official blog post in the description where you'll find links to the GitHub repo, the models, the demo space, and the full technical paper. This is one of the most impressive open- source TTS releases I've seen. The fact that it's competing with and sometimes beating premium closed source models while being free and locally runnable is huge for the AI audio space.
If you found this useful, hit that like button, drop a comment if you try it out, and I'll catch you in the next one.


let's kick things off by taking a look at a set of videos. In this collection, every single video contains incredibly complex movements. Some of you might ask, what model generated these videos? Actually, they were created using Juan 2.2's image to video feature. In our last video, I taught you a specific way to write prompts that effectively enhances character motion.
However, you'll find that the results we're getting today are even more pronounced. So, what technology are we using to achieve this? It is the skills technology that is currently exploding all over the internet. The concept of skills was originally proposed by anthropic. Its ultimate goal is to teach artificial intelligence to acquire more professional capabilities regarding the definition of skills.
I think Google put it very well. They said skills is essentially an open standard designed primarily to extend the capabilities of AI agents. Today we are going to teach you how to handwrite a skill that gives AI professional level capabilities in prompt engineering. The development tool we will be using today is Google's anti-gravity.
Actually this skills technology is closely related to vibe coding or immersive coding. That is why many companies are offering immersive coding tools such as the familiar open coding cloud codec and the anti-gravity tool we are discussing today. You can download the latest version of anti-gravity via their website.
The installation process is very simple. You just log in with your Google account. A quick heads up, if your current network environment requires a proxy to connect to Google, make sure your proxy tool is set to ton mode. I have also built this workflow on running hub. You can access the platform via running hub in the comfio.
Running Hub is a fantastic online workbench because they follow up immediately whenever new models or extensions appear. You can use the invitation link in my video description to register for Running Hub, which will get you thousand free credits. Additionally, logging in daily gives you 100 bonus credits so you can try out your own workflows.
Next, let me introduce how to actually develop a skill. A skill is usually contained within a folder. Inside this folder, there must be a specific file named skill MD. This is a markdown file used to explain which aspect of the AI's capabilities you want to extend. We can clearly see the structure here. So what exactly goes inside this skill MD file.
Pay attention here. First we have a basic description which is its metadata. This metadata is in YAML format. As you can see it includes the name and description indicating what kind of skill this is. Below that there are three very important sections. The first is a detailed description of the skill. The second is defining when to use this skill.
The third is how to use this skill regarding how to use it. There is a keyword here called stepby step which means stepbystep instructions. Essentially, this is a workflow described in natural language. I'll demonstrate this in a moment and you'll understand exactly what step-by-step guidance means. Aside from skill MD, the folder can contain other directories.
For example, scripts. If you want this skill to help you extract frames from a video, you might have some programs or related code inside the scripts folder. Another folder is examples which stores sample files to help the agent better understand your needs. Finally, there is resources where you can put templates and other resource files.
Therefore, skills has a classic description called progressive disclosure of prompts. When AI reads a skill, it is different from reading a standard prompt. A prompt is read all at once, whereas a skill is read only when needed. Once you have a basic understanding of skills, let's teach you how to use them to write prompts that generate sufficient motion effects.
If you have successfully downloaded and installed Google's anti-gravity and logged in, you can launch the development tool. First, click on the agent manager. We need to establish a workspace. Think of this as your working space where all session contacts are centralized. Let's open a new space here. We'll create a new folder named my skills.
Consider this your project directory. Select this folder. Now pay close attention. Inside here, we need to create another folder named agent. Open that and create a folder named skills. This is where you will place your individual skills. Inside here, we'll create a folder and give the skill a name. For example, generate dynamic video prompts.
Open that folder and we will create our markdown file. Note that the name is fixed. We call it skill.md. After opening this file, we paste in the content. This content is developed according to the skill standard we just mentioned. You can think of this as the instruction manual for the skill. Reading this file tells the system how to write these highly efficient dynamic prompts.
Let's look at the first part, the metadata. It contains the skill name and a basic description. Generally, it is set up to analyze a topic or a provided image and then design an action sequence. This sequence contains five actions. These five actions must be logically related and each must be highly recognizable. Below that is the detailed description and the when to use section.
It states that you use this tool when you want to generate a structured image to video prompt. Then comes the specific execution steps. For instance, if we provide a topic or reference image, step one is to analyze the content covering both visual and motion aspects. We need to deeply analyze the subject's movement, center of gravity, and surrounding environment.
Step two is to design the five actions that will occur over 5 seconds. When designing these five actions, we focus on two things. Continuity. The logic must make sense, no jarring jumps and amplitude. We want big movements, not slight ones, so they are recognizable in the video. Step three is to review or optimize the generated actions before the final output.
We must check three aspects. Physical logic, visual distinctiveness, we don't want the actions to look too similar, and overall movement amplitude. Step four focuses on aesthetics. Finally, we provide an output template. This is the scale we've written for you. It looks a lot like a prompt, right? The benefit is that it is an assetized prompt.
Once written, you can use it repeatedly. The AI will automatically decide whether to use this skill and will strictly follow the set steps to execute it one by one. Now, let's demonstrate this. Notice the chat window on the right. I'm going to upload an image I generated earlier. A beauty leaning against a tree in a jungle. Next, I'll write a prompt.
Help me generate an image to video prompt and include dense motion. Let's execute this. After writing the request, we need to select a model. There are many models available. Usually, you can choose Gemini 3 Pro, but since I've used up my quota, I am selecting Gemini 3 flash. Additionally, you can choose between two conversation modes.
The first is plan mode where before generating any actions it will present a task list for you to confirm. The second is fast mode where it decides autonomously and executes without stopping for a task list. I suggest choosing planning here after selecting. Click execute. It has now analyzed that it needs to use this skill and has gone ahead and read the skill.
md file. At this point it generates an execution plan. You can open this plan to review it. If you're unhappy with anything, you can add a comment right there. Just like approving a document saying, "I think we should change this." After editing, click review and it will regenerate the plan until you are satisfied.
Once the plan looks good, click continue. Note that it will notify you after every execution step. So, you can let it run and go do something else. We have now obtained the prompt. Let's copy this prompt. The workflow you see now is a standard one 2.2 imagetovideo workflow. We've used this more than once. So I won't go through the structure again.
I'll upload the reference image, the same one we used for the prompt generation. Paste the prompt and run the workflow. While the workflow is running, let's generate another prompt. This time I'm uploading an image of an Elven beauty. We can switch the model, execute it, and see which one works better. Again, we get a prompt.
You'll notice this prompt is written in much greater detail. Let's look at the results from the two prompts. First, the jungle beauty. Notice that generating five distinct actions in 5 seconds isn't actually very aesthetically pleasing. I did this to clearly demonstrate the effectiveness of this method. In real world usage, two to three actions are sufficient.
The second result is much more beautiful. Look at this elven beauty. She flies up, lands on the ground, and then takes flight again. There are also beautiful fireworks in her hands. Through these two tests, you'll find that not only does it create dense motion, but it also focuses heavily on the rationality of the movement.
Many prompts might generate large, dense movements, but they end up looking very abrupt. We ran quite a few tests. For example, look at this one. A beauty opens the door, gets into a sports car, turns around and drives down the highway. Although there are many actions, each one follows logical progression, and the aesthetic quality is high.
Here is another one, an eastern beauty effect. We've seen similar things on many AI platforms looking very pretty. This is more conventional, but we still emphasize the overall feel and the rationality of the motion. And here is a runaway girl scenario. The sense of drama and conflict is very strong. Having tested all of this, I think you should now have a much clearer understanding of the skills technology.
You will find that it is indeed much more effective than standard prompting. That is all for today. What are you waiting for? Hurry up and try it yourself. Follow me to become someone who truly understands AI.

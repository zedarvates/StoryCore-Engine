# Configuration Ollama avec Gemma 3

## Vue d'ensemble

L'application StoryCore est maintenant configur√©e pour utiliser **Ollama** avec les mod√®les **Gemma 3** en local. Le syst√®me d√©tecte automatiquement les capacit√©s de votre ordinateur et s√©lectionne le meilleur mod√®le.

## Mod√®les Gemma 3 Disponibles

### 1. Gemma 3 1B (L√©ger)
- **ID**: `gemma3:1b`
- **RAM minimum**: 2 GB
- **RAM recommand√©e**: 4 GB
- **VRAM minimum**: 1 GB (si GPU)
- **Description**: Mod√®le le plus petit, r√©ponses rapides, bon pour les t√¢ches basiques
- **Id√©al pour**: Ordinateurs portables, machines avec RAM limit√©e

### 2. Gemma 3 4B (√âquilibr√©) ‚≠ê Recommand√©
- **ID**: `gemma3:4b`
- **RAM minimum**: 6 GB
- **RAM recommand√©e**: 8 GB
- **VRAM minimum**: 3 GB (si GPU)
- **Description**: Mod√®le √©quilibr√©, bonne qualit√© et vitesse
- **Id√©al pour**: La plupart des ordinateurs modernes

### 3. Gemma 3 12B (Puissant)
- **ID**: `gemma3:12b`
- **RAM minimum**: 16 GB
- **RAM recommand√©e**: 24 GB
- **VRAM minimum**: 8 GB (si GPU)
- **Description**: Mod√®le le plus grand, meilleure qualit√©, r√©ponses plus lentes
- **Id√©al pour**: Stations de travail, machines haut de gamme

## Installation d'Ollama

### Windows
1. T√©l√©chargez Ollama depuis [ollama.ai](https://ollama.ai)
2. Installez l'application
3. Ollama d√©marre automatiquement en arri√®re-plan

### macOS
```bash
brew install ollama
ollama serve
```

### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
```

## Installation des Mod√®les Gemma 3

Une fois Ollama install√©, ouvrez un terminal et ex√©cutez:

```bash
# Pour le mod√®le 1B (l√©ger)
ollama pull gemma3:1b

# Pour le mod√®le 4B (recommand√©)
ollama pull gemma3:4b

# Pour le mod√®le 12B (puissant)
ollama pull gemma3:12b
```

## S√©lection Automatique du Mod√®le

L'application d√©tecte automatiquement:
- **RAM totale et disponible**
- **Pr√©sence d'un GPU d√©di√©**
- **VRAM du GPU** (estimation)

Puis s√©lectionne le **meilleur mod√®le** compatible avec votre syst√®me.

### Exemples de S√©lection

| Configuration Syst√®me | Mod√®le S√©lectionn√© | Raison |
|----------------------|-------------------|---------|
| 4 GB RAM, pas de GPU | Gemma 3 1B | RAM limit√©e |
| 8 GB RAM, GPU 4GB | Gemma 3 4B | Configuration √©quilibr√©e |
| 16 GB RAM, GPU 8GB | Gemma 3 12B | Configuration puissante |
| 32 GB RAM, RTX 4090 | Gemma 3 12B | Configuration optimale |

## Configuration dans l'Application

### Initialisation Automatique

Au d√©marrage de l'application:
1. ‚úÖ D√©tection des capacit√©s syst√®me
2. ‚úÖ S√©lection du meilleur mod√®le
3. ‚úÖ V√©rification qu'Ollama est en cours d'ex√©cution
4. ‚úÖ Configuration automatique du service LLM

Vous verrez dans la console:
```
‚úÖ Ollama initialized with Gemma 3 4B
üìç Endpoint: http://localhost:11434
ü§ñ Model: gemma3:4b
üöÄ StoryCore ready with Gemma 3 4B
```

### Configuration Manuelle

Si vous souhaitez changer de mod√®le manuellement:

1. Ouvrez les **Param√®tres** de l'application
2. Allez dans **LLM Configuration**
3. Section **Ollama Settings** affiche:
   - √âtat d'Ollama (en cours / arr√™t√©)
   - Capacit√©s syst√®me d√©tect√©es
   - Mod√®le recommand√©
   - Liste des mod√®les disponibles
4. S√©lectionnez le mod√®le souhait√©
5. Cliquez sur **Appliquer**

## V√©rification du Statut

### Dans l'Application
L'interface affiche:
- üü¢ **Ollama is running** - Tout fonctionne
- üî¥ **Ollama is not running** - Ollama doit √™tre d√©marr√©

### En Ligne de Commande
```bash
# V√©rifier qu'Ollama fonctionne
curl http://localhost:11434/api/tags

# Lister les mod√®les install√©s
ollama list

# Tester un mod√®le
ollama run gemma3:4b "Hello, how are you?"
```

## D√©pannage

### Probl√®me: Ollama n'est pas d√©tect√©
**Solution**:
1. V√©rifiez qu'Ollama est install√©: `ollama --version`
2. D√©marrez Ollama: `ollama serve`
3. V√©rifiez le port: `http://localhost:11434`
4. Cliquez sur **Refresh** dans les param√®tres

### Probl√®me: Mod√®le non install√©
**Solution**:
```bash
# Installez le mod√®le recommand√©
ollama pull gemma3:4b
```

### Probl√®me: R√©ponses lentes
**Solutions**:
1. Utilisez un mod√®le plus petit (gemma3:1b)
2. V√©rifiez que vous avez assez de RAM disponible
3. Fermez les applications gourmandes en m√©moire
4. Si vous avez un GPU, v√©rifiez qu'Ollama l'utilise

### Probl√®me: Erreur de m√©moire
**Solutions**:
1. Utilisez un mod√®le plus petit
2. Augmentez la RAM disponible
3. V√©rifiez les recommandations syst√®me

## Configuration Avanc√©e

### Changer le Port Ollama
Si Ollama utilise un port diff√©rent:

1. Dans les param√®tres, modifiez **Ollama Endpoint**
2. Exemple: `http://localhost:8080`
3. Cliquez sur **Refresh**

### Param√®tres du Mod√®le
Les param√®tres par d√©faut sont:
- **Temperature**: 0.7 (cr√©ativit√© mod√©r√©e)
- **Max Tokens**: 2000 (longueur de r√©ponse)
- **Timeout**: 60 secondes (pour mod√®les locaux)
- **Streaming**: Activ√© (r√©ponses progressives)

## Utilisation dans l'Application

Une fois configur√©, Ollama est utilis√© pour:
- üåç **G√©n√©ration de mondes** (World Wizard)
- üë§ **Cr√©ation de personnages** (Character Wizard)
- üí¨ **G√©n√©ration de dialogues** (Chat Assistant)
- üìù **Suggestions de sc√©narios**
- üé® **Descriptions cr√©atives**

## Performance Attendue

### Gemma 3 1B
- ‚ö° Tr√®s rapide (< 1 seconde par r√©ponse)
- üìä Qualit√©: Basique
- üíæ Utilisation RAM: ~2-3 GB

### Gemma 3 4B
- ‚ö° Rapide (1-3 secondes par r√©ponse)
- üìä Qualit√©: Bonne
- üíæ Utilisation RAM: ~4-6 GB

### Gemma 3 12B
- ‚ö° Mod√©r√© (3-8 secondes par r√©ponse)
- üìä Qualit√©: Excellente
- üíæ Utilisation RAM: ~12-16 GB

## Avantages d'Ollama Local

‚úÖ **Confidentialit√©**: Vos donn√©es restent sur votre machine  
‚úÖ **Pas de co√ªts**: Aucun frais d'API  
‚úÖ **Hors ligne**: Fonctionne sans internet  
‚úÖ **Rapide**: Pas de latence r√©seau  
‚úÖ **Personnalisable**: Contr√¥le total sur les mod√®les  

## Ressources

- **Site Ollama**: [ollama.ai](https://ollama.ai)
- **Documentation**: [github.com/ollama/ollama](https://github.com/ollama/ollama)
- **Mod√®les Gemma**: [ollama.ai/library/gemma3](https://ollama.ai/library/gemma3)
- **Support**: [github.com/ollama/ollama/issues](https://github.com/ollama/ollama/issues)

## Notes Importantes

‚ö†Ô∏è **Premi√®re utilisation**: Le premier t√©l√©chargement d'un mod√®le peut prendre du temps (1-5 GB selon le mod√®le)

‚ö†Ô∏è **RAM**: Assurez-vous d'avoir suffisamment de RAM disponible avant de lancer un mod√®le

‚ö†Ô∏è **GPU**: Si vous avez un GPU NVIDIA, Ollama l'utilisera automatiquement pour acc√©l√©rer les r√©ponses

‚úÖ **Recommandation**: Commencez avec `gemma3:4b` pour un bon √©quilibre qualit√©/performance

# üîß Solution Imm√©diate - Erreur 404 avec Ollama Fonctionnel

## üéØ PROBL√àME IDENTIFI√â

Ollama fonctionne correctement (v√©rifi√© ‚úÖ) et vous avez plusieurs mod√®les install√©s:
- ‚úÖ qwen3-vl:4b
- ‚úÖ gemma3:1b
- ‚úÖ llama3.1:8b
- ‚úÖ Et d'autres...

**MAIS** l'application essaie toujours d'appeler `/api/generate` et re√ßoit une 404.

## üîç CAUSE PROBABLE

La configuration LLM dans l'application pointe vers un mod√®le qui n'existe pas ou la configuration est corrompue.

## ‚úÖ SOLUTION IMM√âDIATE

### √âtape 1: Ouvrir la Console du Navigateur

1. Dans votre navigateur (avec l'application ouverte)
2. Appuyer sur **F12** pour ouvrir les DevTools
3. Aller dans l'onglet **Console**

### √âtape 2: R√©initialiser la Configuration LLM

Copier-coller cette commande dans la console:

```javascript
// Supprimer l'ancienne configuration
localStorage.removeItem('storycore-llm-config');

// Cr√©er une nouvelle configuration avec qwen3-vl:4b
const newConfig = {
  provider: 'local',
  model: 'qwen3-vl:4b',
  apiKey: '',
  apiEndpoint: 'http://localhost:11434',
  streamingEnabled: true,
  parameters: {
    temperature: 0.7,
    maxTokens: 2000,
    topP: 0.9,
    frequencyPenalty: 0,
    presencePenalty: 0
  }
};

localStorage.setItem('storycore-llm-config', JSON.stringify(newConfig));

console.log('‚úÖ Configuration LLM r√©initialis√©e avec qwen3-vl:4b');

// Recharger la page
location.reload();
```

### √âtape 3: V√©rifier dans l'Application

Apr√®s le rechargement:
1. Ouvrir un wizard (World Building)
2. Le banner jaune devrait dispara√Ætre
3. Les boutons de g√©n√©ration AI devraient fonctionner

---

## üîß ALTERNATIVE: Configuration via l'Interface

Si la m√©thode console ne fonctionne pas:

### 1. Ouvrir les Param√®tres LLM

1. Cliquer sur l'ic√¥ne **Settings** (‚öôÔ∏è) dans l'application
2. Aller dans **LLM Configuration**

### 2. Configurer Ollama

- **Provider**: Choisir "Local" ou "Ollama"
- **Endpoint**: `http://localhost:11434`
- **Model**: Taper `qwen3-vl:4b` (ou choisir dans la liste)
- **Temperature**: 0.7
- **Max Tokens**: 2000

### 3. Sauvegarder

Cliquer sur **Save** ou **Sauvegarder**

---

## üß™ TESTER LA CONFIGURATION

### Test 1: V√©rifier Ollama Manuellement

Dans PowerShell:
```powershell
# Tester l'API generate avec qwen3-vl:4b
curl -X POST http://localhost:11434/api/generate -H "Content-Type: application/json" -d '{\"model\":\"qwen3-vl:4b\",\"prompt\":\"Hello\",\"stream\":false}'
```

Si √ßa fonctionne, vous devriez voir une r√©ponse JSON.

### Test 2: V√©rifier dans la Console

Dans la console du navigateur (F12):
```javascript
// V√©rifier la configuration actuelle
const config = JSON.parse(localStorage.getItem('storycore-llm-config'));
console.log('Configuration actuelle:', config);
```

Vous devriez voir:
```javascript
{
  provider: "local",
  model: "qwen3-vl:4b",
  apiEndpoint: "http://localhost:11434",
  ...
}
```

---

## üêõ SI √áA NE FONCTIONNE TOUJOURS PAS

### V√©rifier les Logs de la Console

Dans la console du navigateur, chercher:
```
[LLMProvider] Checking Ollama availability at http://localhost:11434
[LLMProvider] Ollama is available
```

Si vous voyez:
```
[LLMProvider] Ollama is not running or not accessible
```

Alors il y a un probl√®me de connexion.

### V√©rifier le Port d'Ollama

Ollama √©coute normalement sur le port 11434. V√©rifier:

```powershell
# V√©rifier si le port 11434 est ouvert
netstat -an | findstr "11434"
```

Vous devriez voir:
```
TCP    0.0.0.0:11434          0.0.0.0:0              LISTENING
```

---

## üìù COMMANDES RAPIDES

### R√©initialiser la Config (Console Navigateur)
```javascript
localStorage.removeItem('storycore-llm-config');
localStorage.setItem('storycore-llm-config', JSON.stringify({
  provider: 'local',
  model: 'qwen3-vl:4b',
  apiKey: '',
  apiEndpoint: 'http://localhost:11434',
  streamingEnabled: true,
  parameters: { temperature: 0.7, maxTokens: 2000, topP: 0.9, frequencyPenalty: 0, presencePenalty: 0 }
}));
location.reload();
```

### Tester Ollama (PowerShell)
```powershell
# V√©rifier les mod√®les
ollama list

# Tester la g√©n√©ration
ollama run qwen3-vl:4b "Hello, how are you?"

# Tester l'API
curl http://localhost:11434/api/tags
```

---

## ‚úÖ R√âSULTAT ATTENDU

Apr√®s avoir suivi ces √©tapes:
1. ‚úÖ La configuration LLM pointe vers `qwen3-vl:4b`
2. ‚úÖ L'erreur 404 dispara√Æt
3. ‚úÖ Le banner jaune dans les wizards dispara√Æt
4. ‚úÖ Les boutons de g√©n√©ration AI fonctionnent
5. ‚úÖ Vous pouvez utiliser l'assistance LLM

---

**Essayez la m√©thode console en premier (√âtape 2), c'est la plus rapide!**
